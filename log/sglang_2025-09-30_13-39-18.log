/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W0930 13:39:24.192057 2752152 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W0930 13:39:24.192057 2752152 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[2025-09-30 13:39:24] server_args=ServerArgs(model_path='/mnt/data/models/Llama-3.2-3B-Instruct', tokenizer_path='/mnt/data/models/Llama-3.2-3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=30001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.98, max_running_requests=50, max_queued_requests=9223372036854775807, max_total_tokens=64000, chunked_prefill_size=4096, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=164082913, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-customer-labels', tokenizer_metrics_allowed_customer_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='LLAMA', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=True, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=True, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_cutedsl_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-09-30 13:39:25] Using default HuggingFace chat template with detected content format: string
/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W0930 13:39:30.527753 2752453 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W0930 13:39:30.527753 2752453 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
W0930 13:39:30.605758 2752454 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W0930 13:39:30.605758 2752454 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-09-30 13:39:31] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-09-30 13:39:31] Init torch distributed ends. mem usage=0.00 GB
[2025-09-30 13:39:31] CUDA-fused xIELU not available (No module named 'xielu') – falling back to a Python version.
For CUDA xIELU (experimental), `pip install git+https://github.com/nickjbrowning/XIELU`
[2025-09-30 13:39:31] MOE_RUNNER_BACKEND is not initialized, using triton backend
[2025-09-30 13:39:31] Load weight begin. avail mem=23.13 GB
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.26it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.90it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.77it/s]

[2025-09-30 13:39:33] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=17.01 GB, mem usage=6.12 GB.
[2025-09-30 13:39:33] KV Cache is allocated. #tokens: 64000, K size: 3.42 GB, V size: 3.42 GB
[2025-09-30 13:39:33] Memory pool end. avail mem=10.14 GB
[2025-09-30 13:39:33] Capture cuda graph begin. This can take up to several minutes. avail mem=10.10 GB
[2025-09-30 13:39:33] Capture cuda graph bs [1, 2, 4, 8]
  0%|          | 0/4 [00:00<?, ?it/s]Capturing batches (bs=8 avail_mem=10.10 GB):   0%|          | 0/4 [00:00<?, ?it/s]Capturing batches (bs=8 avail_mem=10.10 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.80it/s]Capturing batches (bs=4 avail_mem=10.05 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.80it/s]Capturing batches (bs=2 avail_mem=10.04 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.80it/s]Capturing batches (bs=1 avail_mem=10.04 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.80it/s]Capturing batches (bs=1 avail_mem=10.04 GB): 100%|██████████| 4/4 [00:00<00:00,  7.33it/s]Capturing batches (bs=1 avail_mem=10.04 GB): 100%|██████████| 4/4 [00:00<00:00,  5.95it/s]
[2025-09-30 13:39:34] Capture cuda graph end. Time elapsed: 1.03 s. mem usage=0.07 GB. avail mem=10.04 GB.
[2025-09-30 13:39:34] max_total_num_tokens=64000, chunked_prefill_size=4096, max_prefill_tokens=16384, max_running_requests=50, context_len=131072, available_gpu_mem=10.04 GB
[2025-09-30 13:39:35] INFO:     Started server process [2752152]
[2025-09-30 13:39:35] INFO:     Waiting for application startup.
[2025-09-30 13:39:35] INFO:     Application startup complete.
[2025-09-30 13:39:35] INFO:     Uvicorn running on http://0.0.0.0:30001 (Press CTRL+C to quit)
[2025-09-30 13:39:36] INFO:     127.0.0.1:50184 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-09-30 13:39:36] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:39:36] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-09-30 13:39:36] The server is fired up and ready to roll!
[2025-09-30 13:40:47] Prefill batch. #new-seq: 1, #new-token: 465, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:40:48] Decode batch. #running-req: 1, #token: 499, token usage: 0.01, cuda graph: True, gen throughput (token/s): 0.54, #queue-req: 0, 
[2025-09-30 13:40:48] Decode batch. #running-req: 1, #token: 539, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.06, #queue-req: 0, 
[2025-09-30 13:40:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:40:48] Prefill batch. #new-seq: 1, #new-token: 325, #cached-token: 27, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:40:48] Decode batch. #running-req: 1, #token: 390, token usage: 0.01, cuda graph: True, gen throughput (token/s): 120.99, #queue-req: 0, 
[2025-09-30 13:40:49] Decode batch. #running-req: 1, #token: 430, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.66, #queue-req: 0, 
[2025-09-30 13:40:49] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:40:49] Prefill batch. #new-seq: 1, #new-token: 1280, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:40:50] Decode batch. #running-req: 1, #token: 1326, token usage: 0.02, cuda graph: True, gen throughput (token/s): 41.84, #queue-req: 0, 
[2025-09-30 13:40:50] Decode batch. #running-req: 1, #token: 1366, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.97, #queue-req: 0, 
[2025-09-30 13:40:50] Decode batch. #running-req: 1, #token: 1406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.87, #queue-req: 0, 
[2025-09-30 13:40:51] Decode batch. #running-req: 1, #token: 1446, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.85, #queue-req: 0, 
[2025-09-30 13:40:51] Decode batch. #running-req: 1, #token: 1486, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.82, #queue-req: 0, 
[2025-09-30 13:40:51] Decode batch. #running-req: 1, #token: 1526, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.82, #queue-req: 0, 
[2025-09-30 13:40:51] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:40:51] Prefill batch. #new-seq: 1, #new-token: 1260, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:40:52] Decode batch. #running-req: 1, #token: 1314, token usage: 0.02, cuda graph: True, gen throughput (token/s): 108.24, #queue-req: 0, 
[2025-09-30 13:40:52] Decode batch. #running-req: 1, #token: 1354, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.97, #queue-req: 0, 
[2025-09-30 13:40:52] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:40:52] Prefill batch. #new-seq: 1, #new-token: 1364, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:40:52] Decode batch. #running-req: 1, #token: 1427, token usage: 0.02, cuda graph: True, gen throughput (token/s): 107.20, #queue-req: 0, 
[2025-09-30 13:40:53] Decode batch. #running-req: 1, #token: 1467, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.79, #queue-req: 0, 
[2025-09-30 13:40:53] Decode batch. #running-req: 1, #token: 1507, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.76, #queue-req: 0, 
[2025-09-30 13:40:53] Decode batch. #running-req: 1, #token: 1547, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.62, #queue-req: 0, 
[2025-09-30 13:40:53] Decode batch. #running-req: 1, #token: 1587, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.35, #queue-req: 0, 
[2025-09-30 13:40:54] Decode batch. #running-req: 1, #token: 1627, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.24, #queue-req: 0, 
[2025-09-30 13:40:54] Decode batch. #running-req: 1, #token: 1667, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.19, #queue-req: 0, 
[2025-09-30 13:40:54] Decode batch. #running-req: 1, #token: 1707, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.16, #queue-req: 0, 
[2025-09-30 13:40:55] Decode batch. #running-req: 1, #token: 1747, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.20, #queue-req: 0, 
[2025-09-30 13:40:55] Decode batch. #running-req: 1, #token: 1787, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.16, #queue-req: 0, 
[2025-09-30 13:40:55] Decode batch. #running-req: 1, #token: 1827, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.79, #queue-req: 0, 
[2025-09-30 13:40:56] Decode batch. #running-req: 1, #token: 1867, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.70, #queue-req: 0, 
[2025-09-30 13:40:56] Decode batch. #running-req: 1, #token: 1907, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.64, #queue-req: 0, 
[2025-09-30 13:40:56] Decode batch. #running-req: 1, #token: 1947, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.63, #queue-req: 0, 
[2025-09-30 13:40:57] Decode batch. #running-req: 1, #token: 1987, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.52, #queue-req: 0, 
[2025-09-30 13:40:57] Decode batch. #running-req: 1, #token: 2027, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.52, #queue-req: 0, 
[2025-09-30 13:40:57] Decode batch. #running-req: 1, #token: 2067, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.35, #queue-req: 0, 
[2025-09-30 13:40:57] Decode batch. #running-req: 1, #token: 2107, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.04, #queue-req: 0, 
[2025-09-30 13:40:58] Decode batch. #running-req: 1, #token: 2147, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.00, #queue-req: 0, 
[2025-09-30 13:40:58] Decode batch. #running-req: 1, #token: 2187, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.02, #queue-req: 0, 
[2025-09-30 13:40:58] Decode batch. #running-req: 1, #token: 2227, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.97, #queue-req: 0, 
[2025-09-30 13:40:59] Decode batch. #running-req: 1, #token: 2267, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.91, #queue-req: 0, 
[2025-09-30 13:40:59] Decode batch. #running-req: 1, #token: 2307, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.90, #queue-req: 0, 
[2025-09-30 13:40:59] Decode batch. #running-req: 1, #token: 2347, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.42, #queue-req: 0, 
[2025-09-30 13:41:00] Decode batch. #running-req: 1, #token: 2387, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.39, #queue-req: 0, 
[2025-09-30 13:41:00] Decode batch. #running-req: 1, #token: 2427, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.41, #queue-req: 0, 
[2025-09-30 13:41:00] Decode batch. #running-req: 1, #token: 2467, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.39, #queue-req: 0, 
[2025-09-30 13:41:00] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:01] Prefill batch. #new-seq: 1, #new-token: 967, #cached-token: 29, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:01] Prefill batch. #new-seq: 1, #new-token: 623, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:01] Decode batch. #running-req: 1, #token: 817, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-09-30 13:41:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:01] Prefill batch. #new-seq: 1, #new-token: 72, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:02] Decode batch. #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 119.20, #queue-req: 0, 
[2025-09-30 13:41:02] Decode batch. #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.92, #queue-req: 0, 
[2025-09-30 13:41:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:02] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:02] Decode batch. #running-req: 1, #token: 97, token usage: 0.00, cuda graph: True, gen throughput (token/s): 121.68, #queue-req: 0, 
[2025-09-30 13:41:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:02] Prefill batch. #new-seq: 1, #new-token: 621, #cached-token: 69, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:03] Prefill batch. #new-seq: 1, #new-token: 2526, #cached-token: 339, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:03] Decode batch. #running-req: 1, #token: 2879, token usage: 0.04, cuda graph: True, gen throughput (token/s): 38.93, #queue-req: 0, 
[2025-09-30 13:41:04] Decode batch. #running-req: 1, #token: 2919, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.13, #queue-req: 0, 
[2025-09-30 13:41:04] Decode batch. #running-req: 1, #token: 2959, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.13, #queue-req: 0, 
[2025-09-30 13:41:04] Decode batch. #running-req: 1, #token: 2999, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.11, #queue-req: 0, 
[2025-09-30 13:41:04] Decode batch. #running-req: 1, #token: 3039, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.14, #queue-req: 0, 
[2025-09-30 13:41:05] Decode batch. #running-req: 1, #token: 3079, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.99, #queue-req: 0, 
[2025-09-30 13:41:05] Decode batch. #running-req: 1, #token: 3119, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.61, #queue-req: 0, 
[2025-09-30 13:41:05] Decode batch. #running-req: 1, #token: 3159, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.63, #queue-req: 0, 
[2025-09-30 13:41:06] Decode batch. #running-req: 1, #token: 3199, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.59, #queue-req: 0, 
[2025-09-30 13:41:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:06] Prefill batch. #new-seq: 1, #new-token: 2526, #cached-token: 319, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:06] Decode batch. #running-req: 1, #token: 2848, token usage: 0.04, cuda graph: True, gen throughput (token/s): 90.57, #queue-req: 0, 
[2025-09-30 13:41:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:06] Prefill batch. #new-seq: 1, #new-token: 1329, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:07] Decode batch. #running-req: 1, #token: 1565, token usage: 0.02, cuda graph: True, gen throughput (token/s): 105.17, #queue-req: 0, 
[2025-09-30 13:41:07] Decode batch. #running-req: 1, #token: 1605, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.26, #queue-req: 0, 
[2025-09-30 13:41:07] Decode batch. #running-req: 1, #token: 1645, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.20, #queue-req: 0, 
[2025-09-30 13:41:07] Decode batch. #running-req: 1, #token: 1685, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.17, #queue-req: 0, 
[2025-09-30 13:41:08] Decode batch. #running-req: 1, #token: 1725, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.12, #queue-req: 0, 
[2025-09-30 13:41:08] Decode batch. #running-req: 1, #token: 1765, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.15, #queue-req: 0, 
[2025-09-30 13:41:08] Decode batch. #running-req: 1, #token: 1805, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.99, #queue-req: 0, 
[2025-09-30 13:41:09] Decode batch. #running-req: 1, #token: 1845, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.62, #queue-req: 0, 
[2025-09-30 13:41:09] Decode batch. #running-req: 1, #token: 1885, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.59, #queue-req: 0, 
[2025-09-30 13:41:09] Decode batch. #running-req: 1, #token: 1925, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.56, #queue-req: 0, 
[2025-09-30 13:41:10] Decode batch. #running-req: 1, #token: 1965, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.57, #queue-req: 0, 
[2025-09-30 13:41:10] Decode batch. #running-req: 1, #token: 2005, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.55, #queue-req: 0, 
[2025-09-30 13:41:10] Decode batch. #running-req: 1, #token: 2045, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.54, #queue-req: 0, 
[2025-09-30 13:41:11] Decode batch. #running-req: 1, #token: 2085, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.11, #queue-req: 0, 
[2025-09-30 13:41:11] Decode batch. #running-req: 1, #token: 2125, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.98, #queue-req: 0, 
[2025-09-30 13:41:11] Decode batch. #running-req: 1, #token: 2165, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.90, #queue-req: 0, 
[2025-09-30 13:41:12] Decode batch. #running-req: 1, #token: 2205, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.91, #queue-req: 0, 
[2025-09-30 13:41:12] Decode batch. #running-req: 1, #token: 2245, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.92, #queue-req: 0, 
[2025-09-30 13:41:12] Decode batch. #running-req: 1, #token: 2285, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.90, #queue-req: 0, 
[2025-09-30 13:41:12] Decode batch. #running-req: 1, #token: 2325, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.67, #queue-req: 0, 
[2025-09-30 13:41:13] Decode batch. #running-req: 1, #token: 2365, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.40, #queue-req: 0, 
[2025-09-30 13:41:13] Decode batch. #running-req: 1, #token: 2405, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.38, #queue-req: 0, 
[2025-09-30 13:41:13] Decode batch. #running-req: 1, #token: 2445, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.35, #queue-req: 0, 
[2025-09-30 13:41:14] Decode batch. #running-req: 1, #token: 2485, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.36, #queue-req: 0, 
[2025-09-30 13:41:14] Decode batch. #running-req: 1, #token: 2525, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:41:14] Decode batch. #running-req: 1, #token: 2565, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.25, #queue-req: 0, 
[2025-09-30 13:41:15] Decode batch. #running-req: 1, #token: 2605, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.79, #queue-req: 0, 
[2025-09-30 13:41:15] Decode batch. #running-req: 1, #token: 2645, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.83, #queue-req: 0, 
[2025-09-30 13:41:15] Decode batch. #running-req: 1, #token: 2685, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.83, #queue-req: 0, 
[2025-09-30 13:41:16] Decode batch. #running-req: 1, #token: 2725, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.79, #queue-req: 0, 
[2025-09-30 13:41:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:16] Prefill batch. #new-seq: 1, #new-token: 1320, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:16] Prefill batch. #new-seq: 1, #new-token: 704, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:16] Decode batch. #running-req: 1, #token: 881, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.52, #queue-req: 0, 
[2025-09-30 13:41:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:17] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:17] Decode batch. #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.43, #queue-req: 0, 
[2025-09-30 13:41:17] Decode batch. #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.01, #queue-req: 0, 
[2025-09-30 13:41:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:18] Prefill batch. #new-seq: 1, #new-token: 3891, #cached-token: 340, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:18] Decode batch. #running-req: 1, #token: 4240, token usage: 0.07, cuda graph: True, gen throughput (token/s): 45.16, #queue-req: 0, 
[2025-09-30 13:41:18] Decode batch. #running-req: 1, #token: 4280, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.99, #queue-req: 0, 
[2025-09-30 13:41:19] Decode batch. #running-req: 1, #token: 4320, token usage: 0.07, cuda graph: True, gen throughput (token/s): 123.05, #queue-req: 0, 
[2025-09-30 13:41:19] Decode batch. #running-req: 1, #token: 4360, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.91, #queue-req: 0, 
[2025-09-30 13:41:19] Decode batch. #running-req: 1, #token: 4400, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.64, #queue-req: 0, 
[2025-09-30 13:41:20] Decode batch. #running-req: 1, #token: 4440, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.52, #queue-req: 0, 
[2025-09-30 13:41:20] Decode batch. #running-req: 1, #token: 4480, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.52, #queue-req: 0, 
[2025-09-30 13:41:20] Decode batch. #running-req: 1, #token: 4520, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.57, #queue-req: 0, 
[2025-09-30 13:41:21] Decode batch. #running-req: 1, #token: 4560, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.54, #queue-req: 0, 
[2025-09-30 13:41:21] Decode batch. #running-req: 1, #token: 4600, token usage: 0.07, cuda graph: True, gen throughput (token/s): 122.55, #queue-req: 0, 
[2025-09-30 13:41:21] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:21] Prefill batch. #new-seq: 1, #new-token: 3891, #cached-token: 320, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:21] Decode batch. #running-req: 1, #token: 4225, token usage: 0.07, cuda graph: True, gen throughput (token/s): 73.76, #queue-req: 0, 
[2025-09-30 13:41:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:22] Prefill batch. #new-seq: 1, #new-token: 1331, #cached-token: 385, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:22] Decode batch. #running-req: 1, #token: 1739, token usage: 0.03, cuda graph: True, gen throughput (token/s): 103.52, #queue-req: 0, 
[2025-09-30 13:41:22] Decode batch. #running-req: 1, #token: 1779, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.07, #queue-req: 0, 
[2025-09-30 13:41:22] Decode batch. #running-req: 1, #token: 1819, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.78, #queue-req: 0, 
[2025-09-30 13:41:23] Decode batch. #running-req: 1, #token: 1859, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.58, #queue-req: 0, 
[2025-09-30 13:41:23] Decode batch. #running-req: 1, #token: 1899, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.55, #queue-req: 0, 
[2025-09-30 13:41:23] Decode batch. #running-req: 1, #token: 1939, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.51, #queue-req: 0, 
[2025-09-30 13:41:24] Decode batch. #running-req: 1, #token: 1979, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.53, #queue-req: 0, 
[2025-09-30 13:41:24] Decode batch. #running-req: 1, #token: 2019, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.49, #queue-req: 0, 
[2025-09-30 13:41:24] Decode batch. #running-req: 1, #token: 2059, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.31, #queue-req: 0, 
[2025-09-30 13:41:25] Decode batch. #running-req: 1, #token: 2099, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.00, #queue-req: 0, 
[2025-09-30 13:41:25] Decode batch. #running-req: 1, #token: 2139, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.92, #queue-req: 0, 
[2025-09-30 13:41:25] Decode batch. #running-req: 1, #token: 2179, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.94, #queue-req: 0, 
[2025-09-30 13:41:26] Decode batch. #running-req: 1, #token: 2219, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:41:26] Decode batch. #running-req: 1, #token: 2259, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.88, #queue-req: 0, 
[2025-09-30 13:41:26] Decode batch. #running-req: 1, #token: 2299, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.80, #queue-req: 0, 
[2025-09-30 13:41:27] Decode batch. #running-req: 1, #token: 2339, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.31, #queue-req: 0, 
[2025-09-30 13:41:27] Decode batch. #running-req: 1, #token: 2379, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:41:27] Decode batch. #running-req: 1, #token: 2419, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.26, #queue-req: 0, 
[2025-09-30 13:41:27] Decode batch. #running-req: 1, #token: 2459, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:41:28] Decode batch. #running-req: 1, #token: 2499, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.23, #queue-req: 0, 
[2025-09-30 13:41:28] Decode batch. #running-req: 1, #token: 2539, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.23, #queue-req: 0, 
[2025-09-30 13:41:28] Decode batch. #running-req: 1, #token: 2579, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.02, #queue-req: 0, 
[2025-09-30 13:41:29] Decode batch. #running-req: 1, #token: 2619, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.79, #queue-req: 0, 
[2025-09-30 13:41:29] Decode batch. #running-req: 1, #token: 2659, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.77, #queue-req: 0, 
[2025-09-30 13:41:29] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:30] Prefill batch. #new-seq: 1, #new-token: 1123, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:30] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:30] Prefill batch. #new-seq: 1, #new-token: 427, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:30] Decode batch. #running-req: 1, #token: 624, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.04, #queue-req: 0, 
[2025-09-30 13:41:30] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:30] Prefill batch. #new-seq: 1, #new-token: 81, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:30] Decode batch. #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.36, #queue-req: 0, 
[2025-09-30 13:41:31] Decode batch. #running-req: 1, #token: 338, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.82, #queue-req: 0, 
[2025-09-30 13:41:31] Decode batch. #running-req: 1, #token: 378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.71, #queue-req: 0, 
[2025-09-30 13:41:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:31] Prefill batch. #new-seq: 1, #new-token: 2177, #cached-token: 339, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:32] Decode batch. #running-req: 1, #token: 2555, token usage: 0.04, cuda graph: True, gen throughput (token/s): 49.95, #queue-req: 0, 
[2025-09-30 13:41:32] Decode batch. #running-req: 1, #token: 2595, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.79, #queue-req: 0, 
[2025-09-30 13:41:32] Decode batch. #running-req: 1, #token: 2635, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.68, #queue-req: 0, 
[2025-09-30 13:41:33] Decode batch. #running-req: 1, #token: 2675, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.67, #queue-req: 0, 
[2025-09-30 13:41:33] Decode batch. #running-req: 1, #token: 2715, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.71, #queue-req: 0, 
[2025-09-30 13:41:33] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:33] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.61, #queue-req: 0, 
[2025-09-30 13:41:33] Prefill batch. #new-seq: 1, #new-token: 2177, #cached-token: 319, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:33] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:33] Prefill batch. #new-seq: 1, #new-token: 1149, #cached-token: 125, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:34] Decode batch. #running-req: 1, #token: 1308, token usage: 0.02, cuda graph: True, gen throughput (token/s): 84.04, #queue-req: 0, 
[2025-09-30 13:41:34] Decode batch. #running-req: 1, #token: 1348, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.86, #queue-req: 0, 
[2025-09-30 13:41:34] Decode batch. #running-req: 1, #token: 1388, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.80, #queue-req: 0, 
[2025-09-30 13:41:35] Decode batch. #running-req: 1, #token: 1428, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.71, #queue-req: 0, 
[2025-09-30 13:41:35] Decode batch. #running-req: 1, #token: 1468, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.67, #queue-req: 0, 
[2025-09-30 13:41:35] Decode batch. #running-req: 1, #token: 1508, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.67, #queue-req: 0, 
[2025-09-30 13:41:36] Decode batch. #running-req: 1, #token: 1548, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.59, #queue-req: 0, 
[2025-09-30 13:41:36] Decode batch. #running-req: 1, #token: 1588, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.22, #queue-req: 0, 
[2025-09-30 13:41:36] Decode batch. #running-req: 1, #token: 1628, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.18, #queue-req: 0, 
[2025-09-30 13:41:36] Decode batch. #running-req: 1, #token: 1668, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.15, #queue-req: 0, 
[2025-09-30 13:41:37] Decode batch. #running-req: 1, #token: 1708, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.12, #queue-req: 0, 
[2025-09-30 13:41:37] Decode batch. #running-req: 1, #token: 1748, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.14, #queue-req: 0, 
[2025-09-30 13:41:37] Decode batch. #running-req: 1, #token: 1788, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.11, #queue-req: 0, 
[2025-09-30 13:41:38] Decode batch. #running-req: 1, #token: 1828, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.63, #queue-req: 0, 
[2025-09-30 13:41:38] Decode batch. #running-req: 1, #token: 1868, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.49, #queue-req: 0, 
[2025-09-30 13:41:38] Decode batch. #running-req: 1, #token: 1908, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.37, #queue-req: 0, 
[2025-09-30 13:41:39] Decode batch. #running-req: 1, #token: 1948, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.37, #queue-req: 0, 
[2025-09-30 13:41:39] Decode batch. #running-req: 1, #token: 1988, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.40, #queue-req: 0, 
[2025-09-30 13:41:39] Decode batch. #running-req: 1, #token: 2028, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.36, #queue-req: 0, 
[2025-09-30 13:41:40] Decode batch. #running-req: 1, #token: 2068, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.21, #queue-req: 0, 
[2025-09-30 13:41:40] Decode batch. #running-req: 1, #token: 2108, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.01, #queue-req: 0, 
[2025-09-30 13:41:40] Decode batch. #running-req: 1, #token: 2148, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.95, #queue-req: 0, 
[2025-09-30 13:41:40] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:41] Prefill batch. #new-seq: 1, #new-token: 1008, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:41] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:41] Prefill batch. #new-seq: 1, #new-token: 587, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:41] Decode batch. #running-req: 1, #token: 763, token usage: 0.01, cuda graph: True, gen throughput (token/s): 46.97, #queue-req: 0, 
[2025-09-30 13:41:41] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:41] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:41] Decode batch. #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.45, #queue-req: 0, 
[2025-09-30 13:41:42] Decode batch. #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.87, #queue-req: 0, 
[2025-09-30 13:41:42] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.75, #queue-req: 0, 
[2025-09-30 13:41:42] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:42] Prefill batch. #new-seq: 1, #new-token: 989, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:42] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:42] Prefill batch. #new-seq: 1, #new-token: 408, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:43] Decode batch. #running-req: 1, #token: 609, token usage: 0.01, cuda graph: True, gen throughput (token/s): 51.08, #queue-req: 0, 
[2025-09-30 13:41:43] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:43] Prefill batch. #new-seq: 1, #new-token: 62, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:43] Decode batch. #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.42, #queue-req: 0, 
[2025-09-30 13:41:43] Decode batch. #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.83, #queue-req: 0, 
[2025-09-30 13:41:43] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:44] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:44] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:44] Prefill batch. #new-seq: 1, #new-token: 404, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:44] Decode batch. #running-req: 1, #token: 493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 71.87, #queue-req: 0, 
[2025-09-30 13:41:44] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:44] Prefill batch. #new-seq: 1, #new-token: 889, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:44] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:44] Prefill batch. #new-seq: 1, #new-token: 486, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:45] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:45] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:45] Decode batch. #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.13, #queue-req: 0, 
[2025-09-30 13:41:45] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.87, #queue-req: 0, 
[2025-09-30 13:41:45] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:46] Prefill batch. #new-seq: 1, #new-token: 2983, #cached-token: 340, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:46] Decode batch. #running-req: 1, #token: 3333, token usage: 0.05, cuda graph: True, gen throughput (token/s): 47.65, #queue-req: 0, 
[2025-09-30 13:41:46] Decode batch. #running-req: 1, #token: 3373, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.87, #queue-req: 0, 
[2025-09-30 13:41:46] Decode batch. #running-req: 1, #token: 3413, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.91, #queue-req: 0, 
[2025-09-30 13:41:47] Decode batch. #running-req: 1, #token: 3453, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.85, #queue-req: 0, 
[2025-09-30 13:41:47] Decode batch. #running-req: 1, #token: 3493, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.82, #queue-req: 0, 
[2025-09-30 13:41:47] Decode batch. #running-req: 1, #token: 3533, token usage: 0.06, cuda graph: True, gen throughput (token/s): 124.84, #queue-req: 0, 
[2025-09-30 13:41:48] Decode batch. #running-req: 1, #token: 3573, token usage: 0.06, cuda graph: True, gen throughput (token/s): 124.82, #queue-req: 0, 
[2025-09-30 13:41:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:48] Prefill batch. #new-seq: 1, #new-token: 2983, #cached-token: 320, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:48] Decode batch. #running-req: 1, #token: 3313, token usage: 0.05, cuda graph: True, gen throughput (token/s): 85.59, #queue-req: 0, 
[2025-09-30 13:41:49] Decode batch. #running-req: 1, #token: 3353, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.18, #queue-req: 0, 
[2025-09-30 13:41:49] Decode batch. #running-req: 1, #token: 3393, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.99, #queue-req: 0, 
[2025-09-30 13:41:49] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:49] Prefill batch. #new-seq: 1, #new-token: 899, #cached-token: 419, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:49] Decode batch. #running-req: 1, #token: 1357, token usage: 0.02, cuda graph: True, gen throughput (token/s): 111.47, #queue-req: 0, 
[2025-09-30 13:41:50] Decode batch. #running-req: 1, #token: 1397, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.77, #queue-req: 0, 
[2025-09-30 13:41:50] Decode batch. #running-req: 1, #token: 1437, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.67, #queue-req: 0, 
[2025-09-30 13:41:50] Decode batch. #running-req: 1, #token: 1477, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.64, #queue-req: 0, 
[2025-09-30 13:41:50] Decode batch. #running-req: 1, #token: 1517, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.65, #queue-req: 0, 
[2025-09-30 13:41:51] Decode batch. #running-req: 1, #token: 1557, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.46, #queue-req: 0, 
[2025-09-30 13:41:51] Decode batch. #running-req: 1, #token: 1597, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.17, #queue-req: 0, 
[2025-09-30 13:41:51] Decode batch. #running-req: 1, #token: 1637, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.08, #queue-req: 0, 
[2025-09-30 13:41:52] Decode batch. #running-req: 1, #token: 1677, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.09, #queue-req: 0, 
[2025-09-30 13:41:52] Decode batch. #running-req: 1, #token: 1717, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.07, #queue-req: 0, 
[2025-09-30 13:41:52] Decode batch. #running-req: 1, #token: 1757, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.08, #queue-req: 0, 
[2025-09-30 13:41:53] Decode batch. #running-req: 1, #token: 1797, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.01, #queue-req: 0, 
[2025-09-30 13:41:53] Decode batch. #running-req: 1, #token: 1837, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.58, #queue-req: 0, 
[2025-09-30 13:41:53] Decode batch. #running-req: 1, #token: 1877, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.49, #queue-req: 0, 
[2025-09-30 13:41:54] Decode batch. #running-req: 1, #token: 1917, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:41:54] Decode batch. #running-req: 1, #token: 1957, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.39, #queue-req: 0, 
[2025-09-30 13:41:54] Decode batch. #running-req: 1, #token: 1997, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.33, #queue-req: 0, 
[2025-09-30 13:41:54] Decode batch. #running-req: 1, #token: 2037, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.32, #queue-req: 0, 
[2025-09-30 13:41:55] Decode batch. #running-req: 1, #token: 2077, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.03, #queue-req: 0, 
[2025-09-30 13:41:55] Decode batch. #running-req: 1, #token: 2117, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.86, #queue-req: 0, 
[2025-09-30 13:41:55] Decode batch. #running-req: 1, #token: 2157, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.91, #queue-req: 0, 
[2025-09-30 13:41:56] Decode batch. #running-req: 1, #token: 2197, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.85, #queue-req: 0, 
[2025-09-30 13:41:56] Decode batch. #running-req: 1, #token: 2237, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.78, #queue-req: 0, 
[2025-09-30 13:41:56] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:57] Prefill batch. #new-seq: 1, #new-token: 729, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:57] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:57] Prefill batch. #new-seq: 1, #new-token: 251, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:57] Decode batch. #running-req: 1, #token: 420, token usage: 0.01, cuda graph: True, gen throughput (token/s): 55.15, #queue-req: 0, 
[2025-09-30 13:41:57] Decode batch. #running-req: 1, #token: 460, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.86, #queue-req: 0, 
[2025-09-30 13:41:57] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:57] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:57] Decode batch. #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.45, #queue-req: 0, 
[2025-09-30 13:41:58] Decode batch. #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.91, #queue-req: 0, 
[2025-09-30 13:41:58] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:41:58] Prefill batch. #new-seq: 1, #new-token: 2798, #cached-token: 339, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:41:59] Decode batch. #running-req: 1, #token: 3161, token usage: 0.05, cuda graph: True, gen throughput (token/s): 46.88, #queue-req: 0, 
[2025-09-30 13:41:59] Decode batch. #running-req: 1, #token: 3201, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.43, #queue-req: 0, 
[2025-09-30 13:41:59] Decode batch. #running-req: 1, #token: 3241, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.38, #queue-req: 0, 
[2025-09-30 13:42:00] Decode batch. #running-req: 1, #token: 3281, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.38, #queue-req: 0, 
[2025-09-30 13:42:00] Decode batch. #running-req: 1, #token: 3321, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.46, #queue-req: 0, 
[2025-09-30 13:42:00] Decode batch. #running-req: 1, #token: 3361, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.08, #queue-req: 0, 
[2025-09-30 13:42:00] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:00] Prefill batch. #new-seq: 1, #new-token: 2798, #cached-token: 319, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:01] Decode batch. #running-req: 1, #token: 3124, token usage: 0.05, cuda graph: True, gen throughput (token/s): 87.07, #queue-req: 0, 
[2025-09-30 13:42:01] Decode batch. #running-req: 1, #token: 3164, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.53, #queue-req: 0, 
[2025-09-30 13:42:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:01] Prefill batch. #new-seq: 1, #new-token: 794, #cached-token: 500, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:01] Decode batch. #running-req: 1, #token: 1310, token usage: 0.02, cuda graph: True, gen throughput (token/s): 110.38, #queue-req: 0, 
[2025-09-30 13:42:02] Decode batch. #running-req: 1, #token: 1350, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.83, #queue-req: 0, 
[2025-09-30 13:42:02] Decode batch. #running-req: 1, #token: 1390, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.71, #queue-req: 0, 
[2025-09-30 13:42:02] Decode batch. #running-req: 1, #token: 1430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.65, #queue-req: 0, 
[2025-09-30 13:42:03] Decode batch. #running-req: 1, #token: 1470, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.60, #queue-req: 0, 
[2025-09-30 13:42:03] Decode batch. #running-req: 1, #token: 1510, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.66, #queue-req: 0, 
[2025-09-30 13:42:03] Decode batch. #running-req: 1, #token: 1550, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.47, #queue-req: 0, 
[2025-09-30 13:42:03] Decode batch. #running-req: 1, #token: 1590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.14, #queue-req: 0, 
[2025-09-30 13:42:04] Decode batch. #running-req: 1, #token: 1630, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.08, #queue-req: 0, 
[2025-09-30 13:42:04] Decode batch. #running-req: 1, #token: 1670, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.01, #queue-req: 0, 
[2025-09-30 13:42:04] Decode batch. #running-req: 1, #token: 1710, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.01, #queue-req: 0, 
[2025-09-30 13:42:05] Decode batch. #running-req: 1, #token: 1750, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.01, #queue-req: 0, 
[2025-09-30 13:42:05] Decode batch. #running-req: 1, #token: 1790, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.01, #queue-req: 0, 
[2025-09-30 13:42:05] Decode batch. #running-req: 1, #token: 1830, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.58, #queue-req: 0, 
[2025-09-30 13:42:06] Decode batch. #running-req: 1, #token: 1870, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.38, #queue-req: 0, 
[2025-09-30 13:42:06] Decode batch. #running-req: 1, #token: 1910, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.31, #queue-req: 0, 
[2025-09-30 13:42:06] Decode batch. #running-req: 1, #token: 1950, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.39, #queue-req: 0, 
[2025-09-30 13:42:07] Decode batch. #running-req: 1, #token: 1990, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.31, #queue-req: 0, 
[2025-09-30 13:42:07] Decode batch. #running-req: 1, #token: 2030, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.31, #queue-req: 0, 
[2025-09-30 13:42:07] Decode batch. #running-req: 1, #token: 2070, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.14, #queue-req: 0, 
[2025-09-30 13:42:07] Decode batch. #running-req: 1, #token: 2110, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.91, #queue-req: 0, 
[2025-09-30 13:42:08] Decode batch. #running-req: 1, #token: 2150, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:42:08] Decode batch. #running-req: 1, #token: 2190, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.91, #queue-req: 0, 
[2025-09-30 13:42:08] Decode batch. #running-req: 1, #token: 2230, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:42:09] Decode batch. #running-req: 1, #token: 2270, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.74, #queue-req: 0, 
[2025-09-30 13:42:09] Decode batch. #running-req: 1, #token: 2310, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.74, #queue-req: 0, 
[2025-09-30 13:42:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:10] Prefill batch. #new-seq: 1, #new-token: 729, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:10] Prefill batch. #new-seq: 1, #new-token: 552, #cached-token: 152, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:10] Decode batch. #running-req: 1, #token: 741, token usage: 0.01, cuda graph: True, gen throughput (token/s): 41.57, #queue-req: 0, 
[2025-09-30 13:42:10] Decode batch. #running-req: 1, #token: 781, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.95, #queue-req: 0, 
[2025-09-30 13:42:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:10] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:11] Decode batch. #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.46, #queue-req: 0, 
[2025-09-30 13:42:11] Decode batch. #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.01, #queue-req: 0, 
[2025-09-30 13:42:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:12] Prefill batch. #new-seq: 1, #new-token: 1139, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:12] Prefill batch. #new-seq: 1, #new-token: 659, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:12] Decode batch. #running-req: 1, #token: 837, token usage: 0.01, cuda graph: True, gen throughput (token/s): 39.99, #queue-req: 0, 
[2025-09-30 13:42:12] Decode batch. #running-req: 1, #token: 877, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.41, #queue-req: 0, 
[2025-09-30 13:42:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:12] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:13] Decode batch. #running-req: 1, #token: 237, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.20, #queue-req: 0, 
[2025-09-30 13:42:13] Decode batch. #running-req: 1, #token: 277, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.03, #queue-req: 0, 
[2025-09-30 13:42:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:13] Prefill batch. #new-seq: 1, #new-token: 1204, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:14] Prefill batch. #new-seq: 1, #new-token: 551, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:14] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:14] Decode batch. #running-req: 1, #token: 215, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.28, #queue-req: 0, 
[2025-09-30 13:42:14] Decode batch. #running-req: 1, #token: 255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.25, #queue-req: 0, 
[2025-09-30 13:42:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:15] Prefill batch. #new-seq: 1, #new-token: 1309, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:15] Prefill batch. #new-seq: 1, #new-token: 766, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:15] Decode batch. #running-req: 1, #token: 937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 49.02, #queue-req: 0, 
[2025-09-30 13:42:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:15] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:15] Decode batch. #running-req: 1, #token: 236, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.26, #queue-req: 0, 
[2025-09-30 13:42:16] Decode batch. #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.01, #queue-req: 0, 
[2025-09-30 13:42:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:16] Prefill batch. #new-seq: 1, #new-token: 1176, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:16] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.92, #queue-req: 0, 
[2025-09-30 13:42:16] Prefill batch. #new-seq: 1, #new-token: 416, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:17] Decode batch. #running-req: 1, #token: 620, token usage: 0.01, cuda graph: True, gen throughput (token/s): 120.62, #queue-req: 0, 
[2025-09-30 13:42:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:17] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:17] Decode batch. #running-req: 1, #token: 218, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.61, #queue-req: 0, 
[2025-09-30 13:42:17] Decode batch. #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.15, #queue-req: 0, 
[2025-09-30 13:42:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:18] Prefill batch. #new-seq: 1, #new-token: 748, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:18] Prefill batch. #new-seq: 1, #new-token: 340, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:18] Decode batch. #running-req: 1, #token: 530, token usage: 0.01, cuda graph: True, gen throughput (token/s): 51.32, #queue-req: 0, 
[2025-09-30 13:42:18] Decode batch. #running-req: 1, #token: 570, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.79, #queue-req: 0, 
[2025-09-30 13:42:19] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:19] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:19] Decode batch. #running-req: 1, #token: 222, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.29, #queue-req: 0, 
[2025-09-30 13:42:19] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:19] Prefill batch. #new-seq: 1, #new-token: 718, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:19] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:19] Prefill batch. #new-seq: 1, #new-token: 386, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:19] Decode batch. #running-req: 1, #token: 557, token usage: 0.01, cuda graph: True, gen throughput (token/s): 69.24, #queue-req: 0, 
[2025-09-30 13:42:20] Decode batch. #running-req: 1, #token: 597, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.83, #queue-req: 0, 
[2025-09-30 13:42:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:20] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:20] Decode batch. #running-req: 1, #token: 238, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.64, #queue-req: 0, 
[2025-09-30 13:42:20] Decode batch. #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.05, #queue-req: 0, 
[2025-09-30 13:42:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:20] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:20] Prefill batch. #new-seq: 1, #new-token: 382, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:21] Prefill batch. #new-seq: 1, #new-token: 724, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:21] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:21] Prefill batch. #new-seq: 1, #new-token: 346, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:21] Decode batch. #running-req: 1, #token: 515, token usage: 0.01, cuda graph: True, gen throughput (token/s): 39.00, #queue-req: 0, 
[2025-09-30 13:42:22] Decode batch. #running-req: 1, #token: 555, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.90, #queue-req: 0, 
[2025-09-30 13:42:22] Decode batch. #running-req: 1, #token: 595, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.78, #queue-req: 0, 
[2025-09-30 13:42:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:22] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:22] Decode batch. #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.63, #queue-req: 0, 
[2025-09-30 13:42:22] Decode batch. #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.00, #queue-req: 0, 
[2025-09-30 13:42:23] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:23] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:23] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:23] Prefill batch. #new-seq: 1, #new-token: 340, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:23] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:23] Prefill batch. #new-seq: 1, #new-token: 376, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:23] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-09-30 13:42:23] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:23] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:24] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:24] Decode batch. #running-req: 1, #token: 230, token usage: 0.00, cuda graph: True, gen throughput (token/s): 118.31, #queue-req: 0, 
[2025-09-30 13:42:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:24] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 69, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:24] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:24] Decode batch. #running-req: 1, #token: 113, token usage: 0.00, cuda graph: True, gen throughput (token/s): 72.27, #queue-req: 0, 
[2025-09-30 13:42:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:25] Prefill batch. #new-seq: 1, #new-token: 556, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:25] Prefill batch. #new-seq: 1, #new-token: 523, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:25] Decode batch. #running-req: 1, #token: 721, token usage: 0.01, cuda graph: True, gen throughput (token/s): 51.76, #queue-req: 0, 
[2025-09-30 13:42:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:25] Prefill batch. #new-seq: 1, #new-token: 86, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:25] Decode batch. #running-req: 1, #token: 320, token usage: 0.01, cuda graph: True, gen throughput (token/s): 125.37, #queue-req: 0, 
[2025-09-30 13:42:26] Decode batch. #running-req: 1, #token: 360, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.70, #queue-req: 0, 
[2025-09-30 13:42:26] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:26] Prefill batch. #new-seq: 1, #new-token: 1064, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:26] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:26] Prefill batch. #new-seq: 1, #new-token: 592, #cached-token: 152, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:27] Decode batch. #running-req: 1, #token: 769, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.36, #queue-req: 0, 
[2025-09-30 13:42:27] Decode batch. #running-req: 1, #token: 809, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.74, #queue-req: 0, 
[2025-09-30 13:42:27] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:27] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:27] Decode batch. #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.15, #queue-req: 0, 
[2025-09-30 13:42:27] Decode batch. #running-req: 1, #token: 339, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.81, #queue-req: 0, 
[2025-09-30 13:42:28] Decode batch. #running-req: 1, #token: 379, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.71, #queue-req: 0, 
[2025-09-30 13:42:28] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:28] Prefill batch. #new-seq: 1, #new-token: 2045, #cached-token: 341, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:29] Decode batch. #running-req: 1, #token: 2416, token usage: 0.04, cuda graph: True, gen throughput (token/s): 48.26, #queue-req: 0, 
[2025-09-30 13:42:29] Decode batch. #running-req: 1, #token: 2456, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.26, #queue-req: 0, 
[2025-09-30 13:42:29] Decode batch. #running-req: 1, #token: 2496, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.25, #queue-req: 0, 
[2025-09-30 13:42:30] Decode batch. #running-req: 1, #token: 2536, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.16, #queue-req: 0, 
[2025-09-30 13:42:30] Decode batch. #running-req: 1, #token: 2576, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.07, #queue-req: 0, 
[2025-09-30 13:42:30] Decode batch. #running-req: 1, #token: 2616, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.77, #queue-req: 0, 
[2025-09-30 13:42:30] Decode batch. #running-req: 1, #token: 2656, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.73, #queue-req: 0, 
[2025-09-30 13:42:31] Decode batch. #running-req: 1, #token: 2696, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.73, #queue-req: 0, 
[2025-09-30 13:42:31] Decode batch. #running-req: 1, #token: 2736, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.67, #queue-req: 0, 
[2025-09-30 13:42:31] Decode batch. #running-req: 1, #token: 2776, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.70, #queue-req: 0, 
[2025-09-30 13:42:32] Decode batch. #running-req: 1, #token: 2816, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.62, #queue-req: 0, 
[2025-09-30 13:42:32] Decode batch. #running-req: 1, #token: 2856, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.11, #queue-req: 0, 
[2025-09-30 13:42:32] Decode batch. #running-req: 1, #token: 2896, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.10, #queue-req: 0, 
[2025-09-30 13:42:33] Decode batch. #running-req: 1, #token: 2936, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.08, #queue-req: 0, 
[2025-09-30 13:42:33] Decode batch. #running-req: 1, #token: 2976, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.13, #queue-req: 0, 
[2025-09-30 13:42:33] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:33] Prefill batch. #new-seq: 1, #new-token: 2045, #cached-token: 321, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:33] Decode batch. #running-req: 1, #token: 2379, token usage: 0.04, cuda graph: True, gen throughput (token/s): 96.42, #queue-req: 0, 
[2025-09-30 13:42:34] Decode batch. #running-req: 1, #token: 2419, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.31, #queue-req: 0, 
[2025-09-30 13:42:34] Decode batch. #running-req: 1, #token: 2459, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:42:34] Decode batch. #running-req: 1, #token: 2499, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:42:35] Decode batch. #running-req: 1, #token: 2539, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.25, #queue-req: 0, 
[2025-09-30 13:42:35] Decode batch. #running-req: 1, #token: 2579, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.08, #queue-req: 0, 
[2025-09-30 13:42:35] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:35] Prefill batch. #new-seq: 1, #new-token: 1323, #cached-token: 396, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:35] Decode batch. #running-req: 1, #token: 1758, token usage: 0.03, cuda graph: True, gen throughput (token/s): 106.42, #queue-req: 0, 
[2025-09-30 13:42:36] Decode batch. #running-req: 1, #token: 1798, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.02, #queue-req: 0, 
[2025-09-30 13:42:36] Decode batch. #running-req: 1, #token: 1838, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.58, #queue-req: 0, 
[2025-09-30 13:42:36] Decode batch. #running-req: 1, #token: 1878, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.48, #queue-req: 0, 
[2025-09-30 13:42:37] Decode batch. #running-req: 1, #token: 1918, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.49, #queue-req: 0, 
[2025-09-30 13:42:37] Decode batch. #running-req: 1, #token: 1958, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.49, #queue-req: 0, 
[2025-09-30 13:42:37] Decode batch. #running-req: 1, #token: 1998, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:42:38] Decode batch. #running-req: 1, #token: 2038, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:42:38] Decode batch. #running-req: 1, #token: 2078, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.06, #queue-req: 0, 
[2025-09-30 13:42:38] Decode batch. #running-req: 1, #token: 2118, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.90, #queue-req: 0, 
[2025-09-30 13:42:39] Decode batch. #running-req: 1, #token: 2158, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:42:39] Decode batch. #running-req: 1, #token: 2198, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.86, #queue-req: 0, 
[2025-09-30 13:42:39] Decode batch. #running-req: 1, #token: 2238, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.86, #queue-req: 0, 
[2025-09-30 13:42:39] Decode batch. #running-req: 1, #token: 2278, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.79, #queue-req: 0, 
[2025-09-30 13:42:40] Decode batch. #running-req: 1, #token: 2318, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.67, #queue-req: 0, 
[2025-09-30 13:42:40] Decode batch. #running-req: 1, #token: 2358, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.38, #queue-req: 0, 
[2025-09-30 13:42:40] Decode batch. #running-req: 1, #token: 2398, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.34, #queue-req: 0, 
[2025-09-30 13:42:41] Decode batch. #running-req: 1, #token: 2438, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.26, #queue-req: 0, 
[2025-09-30 13:42:41] Decode batch. #running-req: 1, #token: 2478, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.10, #queue-req: 0, 
[2025-09-30 13:42:41] Decode batch. #running-req: 1, #token: 2518, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:42:42] Decode batch. #running-req: 1, #token: 2558, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.19, #queue-req: 0, 
[2025-09-30 13:42:42] Decode batch. #running-req: 1, #token: 2598, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.79, #queue-req: 0, 
[2025-09-30 13:42:42] Decode batch. #running-req: 1, #token: 2638, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.76, #queue-req: 0, 
[2025-09-30 13:42:43] Decode batch. #running-req: 1, #token: 2678, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.72, #queue-req: 0, 
[2025-09-30 13:42:43] Decode batch. #running-req: 1, #token: 2718, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.70, #queue-req: 0, 
[2025-09-30 13:42:43] Decode batch. #running-req: 1, #token: 2758, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.73, #queue-req: 0, 
[2025-09-30 13:42:44] Decode batch. #running-req: 1, #token: 2798, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.69, #queue-req: 0, 
[2025-09-30 13:42:44] Decode batch. #running-req: 1, #token: 2838, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.32, #queue-req: 0, 
[2025-09-30 13:42:44] Decode batch. #running-req: 1, #token: 2878, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.12, #queue-req: 0, 
[2025-09-30 13:42:44] Decode batch. #running-req: 1, #token: 2918, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.12, #queue-req: 0, 
[2025-09-30 13:42:45] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:45] Prefill batch. #new-seq: 1, #new-token: 905, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:45] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:45] Prefill batch. #new-seq: 1, #new-token: 362, #cached-token: 165, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:45] Decode batch. #running-req: 1, #token: 536, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.39, #queue-req: 0, 
[2025-09-30 13:42:46] Decode batch. #running-req: 1, #token: 576, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.80, #queue-req: 0, 
[2025-09-30 13:42:46] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:46] Prefill batch. #new-seq: 1, #new-token: 70, #cached-token: 198, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:46] Decode batch. #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:42:46] Decode batch. #running-req: 1, #token: 335, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.79, #queue-req: 0, 
[2025-09-30 13:42:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:47] Prefill batch. #new-seq: 1, #new-token: 651, #cached-token: 80, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:47] Prefill batch. #new-seq: 1, #new-token: 291, #cached-token: 167, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:47] Decode batch. #running-req: 1, #token: 463, token usage: 0.01, cuda graph: True, gen throughput (token/s): 51.13, #queue-req: 0, 
[2025-09-30 13:42:47] Decode batch. #running-req: 1, #token: 503, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.43, #queue-req: 0, 
[2025-09-30 13:42:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:48] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:48] Decode batch. #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.41, #queue-req: 0, 
[2025-09-30 13:42:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:48] Prefill batch. #new-seq: 1, #new-token: 1447, #cached-token: 341, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:48] Decode batch. #running-req: 1, #token: 1798, token usage: 0.03, cuda graph: True, gen throughput (token/s): 68.97, #queue-req: 0, 
[2025-09-30 13:42:49] Decode batch. #running-req: 1, #token: 1838, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:42:49] Decode batch. #running-req: 1, #token: 1878, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:42:49] Decode batch. #running-req: 1, #token: 1918, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.46, #queue-req: 0, 
[2025-09-30 13:42:50] Decode batch. #running-req: 1, #token: 1958, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.42, #queue-req: 0, 
[2025-09-30 13:42:50] Decode batch. #running-req: 1, #token: 1998, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.38, #queue-req: 0, 
[2025-09-30 13:42:50] Decode batch. #running-req: 1, #token: 2038, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.41, #queue-req: 0, 
[2025-09-30 13:42:50] Decode batch. #running-req: 1, #token: 2078, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.07, #queue-req: 0, 
[2025-09-30 13:42:51] Decode batch. #running-req: 1, #token: 2118, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:42:51] Decode batch. #running-req: 1, #token: 2158, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.90, #queue-req: 0, 
[2025-09-30 13:42:51] Decode batch. #running-req: 1, #token: 2198, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:42:52] Decode batch. #running-req: 1, #token: 2238, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.81, #queue-req: 0, 
[2025-09-30 13:42:52] Decode batch. #running-req: 1, #token: 2278, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.81, #queue-req: 0, 
[2025-09-30 13:42:52] Decode batch. #running-req: 1, #token: 2318, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.67, #queue-req: 0, 
[2025-09-30 13:42:53] Decode batch. #running-req: 1, #token: 2358, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.31, #queue-req: 0, 
[2025-09-30 13:42:53] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:53] Prefill batch. #new-seq: 1, #new-token: 1447, #cached-token: 321, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:53] Decode batch. #running-req: 1, #token: 1780, token usage: 0.03, cuda graph: True, gen throughput (token/s): 103.35, #queue-req: 0, 
[2025-09-30 13:42:53] Decode batch. #running-req: 1, #token: 1820, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.67, #queue-req: 0, 
[2025-09-30 13:42:54] Decode batch. #running-req: 1, #token: 1860, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.49, #queue-req: 0, 
[2025-09-30 13:42:54] Decode batch. #running-req: 1, #token: 1900, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:42:54] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:42:54] Prefill batch. #new-seq: 1, #new-token: 1603, #cached-token: 261, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:42:54] Decode batch. #running-req: 1, #token: 1888, token usage: 0.03, cuda graph: True, gen throughput (token/s): 102.04, #queue-req: 0, 
[2025-09-30 13:42:55] Decode batch. #running-req: 1, #token: 1928, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.34, #queue-req: 0, 
[2025-09-30 13:42:55] Decode batch. #running-req: 1, #token: 1968, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.35, #queue-req: 0, 
[2025-09-30 13:42:55] Decode batch. #running-req: 1, #token: 2008, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.27, #queue-req: 0, 
[2025-09-30 13:42:56] Decode batch. #running-req: 1, #token: 2048, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.28, #queue-req: 0, 
[2025-09-30 13:42:56] Decode batch. #running-req: 1, #token: 2088, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.84, #queue-req: 0, 
[2025-09-30 13:42:56] Decode batch. #running-req: 1, #token: 2128, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.81, #queue-req: 0, 
[2025-09-30 13:42:57] Decode batch. #running-req: 1, #token: 2168, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.73, #queue-req: 0, 
[2025-09-30 13:42:57] Decode batch. #running-req: 1, #token: 2208, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.70, #queue-req: 0, 
[2025-09-30 13:42:57] Decode batch. #running-req: 1, #token: 2248, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.71, #queue-req: 0, 
[2025-09-30 13:42:57] Decode batch. #running-req: 1, #token: 2288, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.71, #queue-req: 0, 
[2025-09-30 13:42:58] Decode batch. #running-req: 1, #token: 2328, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.44, #queue-req: 0, 
[2025-09-30 13:42:58] Decode batch. #running-req: 1, #token: 2368, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:42:58] Decode batch. #running-req: 1, #token: 2408, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:42:59] Decode batch. #running-req: 1, #token: 2448, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.18, #queue-req: 0, 
[2025-09-30 13:42:59] Decode batch. #running-req: 1, #token: 2488, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.07, #queue-req: 0, 
[2025-09-30 13:42:59] Decode batch. #running-req: 1, #token: 2528, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.04, #queue-req: 0, 
[2025-09-30 13:43:00] Decode batch. #running-req: 1, #token: 2568, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.01, #queue-req: 0, 
[2025-09-30 13:43:00] Decode batch. #running-req: 1, #token: 2608, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.59, #queue-req: 0, 
[2025-09-30 13:43:00] Decode batch. #running-req: 1, #token: 2648, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.60, #queue-req: 0, 
[2025-09-30 13:43:01] Decode batch. #running-req: 1, #token: 2688, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.49, #queue-req: 0, 
[2025-09-30 13:43:01] Decode batch. #running-req: 1, #token: 2728, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.55, #queue-req: 0, 
[2025-09-30 13:43:01] Decode batch. #running-req: 1, #token: 2768, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.49, #queue-req: 0, 
[2025-09-30 13:43:02] Decode batch. #running-req: 1, #token: 2808, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.50, #queue-req: 0, 
[2025-09-30 13:43:02] Decode batch. #running-req: 1, #token: 2848, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.10, #queue-req: 0, 
[2025-09-30 13:43:02] Decode batch. #running-req: 1, #token: 2888, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.99, #queue-req: 0, 
[2025-09-30 13:43:03] Decode batch. #running-req: 1, #token: 2928, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.94, #queue-req: 0, 
[2025-09-30 13:43:03] Decode batch. #running-req: 1, #token: 2968, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.88, #queue-req: 0, 
[2025-09-30 13:43:03] Decode batch. #running-req: 1, #token: 3008, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.89, #queue-req: 0, 
[2025-09-30 13:43:03] Decode batch. #running-req: 1, #token: 3048, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.87, #queue-req: 0, 
[2025-09-30 13:43:04] Decode batch. #running-req: 1, #token: 3088, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.71, #queue-req: 0, 
[2025-09-30 13:43:04] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:05] Prefill batch. #new-seq: 1, #new-token: 502, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:05] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:05] Prefill batch. #new-seq: 1, #new-token: 289, #cached-token: 153, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:05] Decode batch. #running-req: 1, #token: 453, token usage: 0.01, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-09-30 13:43:05] Decode batch. #running-req: 1, #token: 493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.29, #queue-req: 0, 
[2025-09-30 13:43:06] Decode batch. #running-req: 1, #token: 533, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.16, #queue-req: 0, 
[2025-09-30 13:43:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:06] Prefill batch. #new-seq: 1, #new-token: 47, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:06] Decode batch. #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.82, #queue-req: 0, 
[2025-09-30 13:43:06] Decode batch. #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:43:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:07] Prefill batch. #new-seq: 1, #new-token: 504, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:07] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:07] Prefill batch. #new-seq: 1, #new-token: 293, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:07] Decode batch. #running-req: 1, #token: 480, token usage: 0.01, cuda graph: True, gen throughput (token/s): 51.92, #queue-req: 0, 
[2025-09-30 13:43:07] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:07] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:07] Decode batch. #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.66, #queue-req: 0, 
[2025-09-30 13:43:08] Decode batch. #running-req: 1, #token: 343, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.79, #queue-req: 0, 
[2025-09-30 13:43:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:08] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.66, #queue-req: 0, 
[2025-09-30 13:43:08] Prefill batch. #new-seq: 1, #new-token: 767, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:08] Prefill batch. #new-seq: 1, #new-token: 481, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:09] Prefill batch. #new-seq: 1, #new-token: 75, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:09] Decode batch. #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.12, #queue-req: 0, 
[2025-09-30 13:43:09] Decode batch. #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.77, #queue-req: 0, 
[2025-09-30 13:43:09] Decode batch. #running-req: 1, #token: 358, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.76, #queue-req: 0, 
[2025-09-30 13:43:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:10] Prefill batch. #new-seq: 1, #new-token: 1174, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:10] Prefill batch. #new-seq: 1, #new-token: 699, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:10] Decode batch. #running-req: 1, #token: 895, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.70, #queue-req: 0, 
[2025-09-30 13:43:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:10] Prefill batch. #new-seq: 1, #new-token: 65, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:10] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.13, #queue-req: 0, 
[2025-09-30 13:43:11] Decode batch. #running-req: 1, #token: 340, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.77, #queue-req: 0, 
[2025-09-30 13:43:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:11] Prefill batch. #new-seq: 1, #new-token: 1169, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:11] Prefill batch. #new-seq: 1, #new-token: 476, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:12] Decode batch. #running-req: 1, #token: 650, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-09-30 13:43:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:12] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:12] Decode batch. #running-req: 1, #token: 233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.67, #queue-req: 0, 
[2025-09-30 13:43:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:12] Prefill batch. #new-seq: 1, #new-token: 980, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:12] Prefill batch. #new-seq: 1, #new-token: 511, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:13] Decode batch. #running-req: 1, #token: 684, token usage: 0.01, cuda graph: True, gen throughput (token/s): 66.99, #queue-req: 0, 
[2025-09-30 13:43:13] Decode batch. #running-req: 1, #token: 724, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.03, #queue-req: 0, 
[2025-09-30 13:43:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:13] Prefill batch. #new-seq: 1, #new-token: 32, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:13] Decode batch. #running-req: 1, #token: 256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.28, #queue-req: 0, 
[2025-09-30 13:43:13] Decode batch. #running-req: 1, #token: 296, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.83, #queue-req: 0, 
[2025-09-30 13:43:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:14] Prefill batch. #new-seq: 1, #new-token: 1088, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:14] Prefill batch. #new-seq: 1, #new-token: 578, #cached-token: 168, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:14] Decode batch. #running-req: 1, #token: 771, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.56, #queue-req: 0, 
[2025-09-30 13:43:15] Decode batch. #running-req: 1, #token: 811, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.70, #queue-req: 0, 
[2025-09-30 13:43:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:15] Prefill batch. #new-seq: 1, #new-token: 60, #cached-token: 201, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:15] Decode batch. #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.06, #queue-req: 0, 
[2025-09-30 13:43:15] Decode batch. #running-req: 1, #token: 322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.90, #queue-req: 0, 
[2025-09-30 13:43:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:15] Prefill batch. #new-seq: 1, #new-token: 797, #cached-token: 83, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:15] Prefill batch. #new-seq: 1, #new-token: 227, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:16] Decode batch. #running-req: 1, #token: 402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 110.07, #queue-req: 0, 
[2025-09-30 13:43:16] Decode batch. #running-req: 1, #token: 442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.84, #queue-req: 0, 
[2025-09-30 13:43:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:16] Prefill batch. #new-seq: 1, #new-token: 63, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:16] Decode batch. #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.77, #queue-req: 0, 
[2025-09-30 13:43:16] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.86, #queue-req: 0, 
[2025-09-30 13:43:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:17] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:17] Decode batch. #running-req: 1, #token: 93, token usage: 0.00, cuda graph: True, gen throughput (token/s): 74.09, #queue-req: 0, 
[2025-09-30 13:43:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:17] Prefill batch. #new-seq: 1, #new-token: 223, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:18] Prefill batch. #new-seq: 1, #new-token: 408, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:18] Prefill batch. #new-seq: 1, #new-token: 189, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:18] Decode batch. #running-req: 1, #token: 366, token usage: 0.01, cuda graph: True, gen throughput (token/s): 50.74, #queue-req: 0, 
[2025-09-30 13:43:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:18] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:18] Decode batch. #running-req: 1, #token: 232, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.22, #queue-req: 0, 
[2025-09-30 13:43:18] Decode batch. #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.01, #queue-req: 0, 
[2025-09-30 13:43:19] Decode batch. #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.88, #queue-req: 0, 
[2025-09-30 13:43:19] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:19] Prefill batch. #new-seq: 1, #new-token: 323, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:19] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:19] Prefill batch. #new-seq: 1, #new-token: 194, #cached-token: 152, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:19] Decode batch. #running-req: 1, #token: 372, token usage: 0.01, cuda graph: True, gen throughput (token/s): 51.27, #queue-req: 0, 
[2025-09-30 13:43:20] Decode batch. #running-req: 1, #token: 412, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.95, #queue-req: 0, 
[2025-09-30 13:43:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:20] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:20] Decode batch. #running-req: 1, #token: 242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.80, #queue-req: 0, 
[2025-09-30 13:43:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:20] Prefill batch. #new-seq: 1, #new-token: 663, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:21] Prefill batch. #new-seq: 1, #new-token: 531, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:21] Decode batch. #running-req: 1, #token: 715, token usage: 0.01, cuda graph: True, gen throughput (token/s): 67.46, #queue-req: 0, 
[2025-09-30 13:43:21] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:21] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:21] Decode batch. #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.68, #queue-req: 0, 
[2025-09-30 13:43:21] Decode batch. #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:43:21] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:22] Prefill batch. #new-seq: 1, #new-token: 1148, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:22] Prefill batch. #new-seq: 1, #new-token: 625, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:22] Decode batch. #running-req: 1, #token: 812, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.47, #queue-req: 0, 
[2025-09-30 13:43:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:22] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:22] Decode batch. #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.53, #queue-req: 0, 
[2025-09-30 13:43:23] Decode batch. #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.02, #queue-req: 0, 
[2025-09-30 13:43:23] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:23] Prefill batch. #new-seq: 1, #new-token: 887, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:24] Prefill batch. #new-seq: 1, #new-token: 270, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:24] Decode batch. #running-req: 1, #token: 433, token usage: 0.01, cuda graph: True, gen throughput (token/s): 49.98, #queue-req: 0, 
[2025-09-30 13:43:24] Decode batch. #running-req: 1, #token: 473, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.53, #queue-req: 0, 
[2025-09-30 13:43:24] Decode batch. #running-req: 1, #token: 513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.30, #queue-req: 0, 
[2025-09-30 13:43:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:24] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:24] Decode batch. #running-req: 1, #token: 222, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.28, #queue-req: 0, 
[2025-09-30 13:43:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:25] Prefill batch. #new-seq: 1, #new-token: 629, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:25] Prefill batch. #new-seq: 1, #new-token: 363, #cached-token: 165, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:25] Decode batch. #running-req: 1, #token: 533, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.20, #queue-req: 0, 
[2025-09-30 13:43:25] Decode batch. #running-req: 1, #token: 573, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.82, #queue-req: 0, 
[2025-09-30 13:43:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:25] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 199, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:25] Decode batch. #running-req: 1, #token: 233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.81, #queue-req: 0, 
[2025-09-30 13:43:26] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:26] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:26] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:26] Prefill batch. #new-seq: 1, #new-token: 361, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:26] Decode batch. #running-req: 1, #token: 446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 70.85, #queue-req: 0, 
[2025-09-30 13:43:26] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:27] Prefill batch. #new-seq: 1, #new-token: 754, #cached-token: 80, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:27] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:27] Prefill batch. #new-seq: 1, #new-token: 387, #cached-token: 173, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:27] Decode batch. #running-req: 1, #token: 588, token usage: 0.01, cuda graph: True, gen throughput (token/s): 50.33, #queue-req: 0, 
[2025-09-30 13:43:27] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:27] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 199, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:27] Decode batch. #running-req: 1, #token: 212, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.98, #queue-req: 0, 
[2025-09-30 13:43:27] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:27] Prefill batch. #new-seq: 1, #new-token: 743, #cached-token: 88, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:27] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:27] Prefill batch. #new-seq: 1, #new-token: 363, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:28] Decode batch. #running-req: 1, #token: 546, token usage: 0.01, cuda graph: True, gen throughput (token/s): 108.95, #queue-req: 0, 
[2025-09-30 13:43:28] Decode batch. #running-req: 1, #token: 586, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.46, #queue-req: 0, 
[2025-09-30 13:43:28] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:28] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 131.20, #queue-req: 0, 
[2025-09-30 13:43:28] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:28] Decode batch. #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.84, #queue-req: 0, 
[2025-09-30 13:43:29] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:29] Prefill batch. #new-seq: 1, #new-token: 579, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:29] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.60, #queue-req: 0, 
[2025-09-30 13:43:29] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:29] Prefill batch. #new-seq: 1, #new-token: 224, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:30] Decode batch. #running-req: 1, #token: 425, token usage: 0.01, cuda graph: True, gen throughput (token/s): 124.63, #queue-req: 0, 
[2025-09-30 13:43:30] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:30] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:30] Decode batch. #running-req: 1, #token: 247, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.17, #queue-req: 0, 
[2025-09-30 13:43:30] Decode batch. #running-req: 1, #token: 287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.91, #queue-req: 0, 
[2025-09-30 13:43:30] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:31] Prefill batch. #new-seq: 1, #new-token: 566, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:31] Prefill batch. #new-seq: 1, #new-token: 416, #cached-token: 152, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:31] Decode batch. #running-req: 1, #token: 575, token usage: 0.01, cuda graph: True, gen throughput (token/s): 49.55, #queue-req: 0, 
[2025-09-30 13:43:31] Decode batch. #running-req: 1, #token: 615, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.55, #queue-req: 0, 
[2025-09-30 13:43:32] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:32] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:32] Decode batch. #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.91, #queue-req: 0, 
[2025-09-30 13:43:32] Decode batch. #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.96, #queue-req: 0, 
[2025-09-30 13:43:32] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:33] Prefill batch. #new-seq: 1, #new-token: 555, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:33] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:33] Prefill batch. #new-seq: 1, #new-token: 215, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:33] Decode batch. #running-req: 1, #token: 382, token usage: 0.01, cuda graph: True, gen throughput (token/s): 50.41, #queue-req: 0, 
[2025-09-30 13:43:33] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:33] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:33] Decode batch. #running-req: 1, #token: 209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.29, #queue-req: 0, 
[2025-09-30 13:43:33] Decode batch. #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.18, #queue-req: 0, 
[2025-09-30 13:43:34] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:34] Prefill batch. #new-seq: 1, #new-token: 489, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:34] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:34] Prefill batch. #new-seq: 1, #new-token: 282, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:34] Decode batch. #running-req: 1, #token: 446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 50.48, #queue-req: 0, 
[2025-09-30 13:43:34] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:34] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:34] Decode batch. #running-req: 1, #token: 213, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.03, #queue-req: 0, 
[2025-09-30 13:43:35] Decode batch. #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.24, #queue-req: 0, 
[2025-09-30 13:43:35] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:35] Prefill batch. #new-seq: 1, #new-token: 487, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:35] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:35] Prefill batch. #new-seq: 1, #new-token: 212, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:36] Decode batch. #running-req: 1, #token: 388, token usage: 0.01, cuda graph: True, gen throughput (token/s): 50.35, #queue-req: 0, 
[2025-09-30 13:43:36] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:36] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:36] Decode batch. #running-req: 1, #token: 216, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.63, #queue-req: 0, 
[2025-09-30 13:43:36] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:36] Prefill batch. #new-seq: 1, #new-token: 438, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:36] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:36] Prefill batch. #new-seq: 1, #new-token: 234, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:36] Decode batch. #running-req: 1, #token: 403, token usage: 0.01, cuda graph: True, gen throughput (token/s): 68.66, #queue-req: 0, 
[2025-09-30 13:43:37] Decode batch. #running-req: 1, #token: 443, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.76, #queue-req: 0, 
[2025-09-30 13:43:37] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:37] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:37] Decode batch. #running-req: 1, #token: 244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.18, #queue-req: 0, 
[2025-09-30 13:43:37] Decode batch. #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.95, #queue-req: 0, 
[2025-09-30 13:43:37] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:38] Prefill batch. #new-seq: 1, #new-token: 1526, #cached-token: 339, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:38] Decode batch. #running-req: 1, #token: 1893, token usage: 0.03, cuda graph: True, gen throughput (token/s): 48.05, #queue-req: 0, 
[2025-09-30 13:43:38] Decode batch. #running-req: 1, #token: 1933, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.51, #queue-req: 0, 
[2025-09-30 13:43:39] Decode batch. #running-req: 1, #token: 1973, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.52, #queue-req: 0, 
[2025-09-30 13:43:39] Decode batch. #running-req: 1, #token: 2013, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.47, #queue-req: 0, 
[2025-09-30 13:43:39] Decode batch. #running-req: 1, #token: 2053, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.38, #queue-req: 0, 
[2025-09-30 13:43:40] Decode batch. #running-req: 1, #token: 2093, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.96, #queue-req: 0, 
[2025-09-30 13:43:40] Decode batch. #running-req: 1, #token: 2133, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.93, #queue-req: 0, 
[2025-09-30 13:43:40] Decode batch. #running-req: 1, #token: 2173, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:43:41] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:41] Prefill batch. #new-seq: 1, #new-token: 1526, #cached-token: 319, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:41] Decode batch. #running-req: 1, #token: 1849, token usage: 0.03, cuda graph: True, gen throughput (token/s): 102.64, #queue-req: 0, 
[2025-09-30 13:43:41] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:41] Prefill batch. #new-seq: 1, #new-token: 1616, #cached-token: 125, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:41] Decode batch. #running-req: 1, #token: 1745, token usage: 0.03, cuda graph: True, gen throughput (token/s): 102.29, #queue-req: 0, 
[2025-09-30 13:43:41] Decode batch. #running-req: 1, #token: 1785, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.93, #queue-req: 0, 
[2025-09-30 13:43:42] Decode batch. #running-req: 1, #token: 1825, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.67, #queue-req: 0, 
[2025-09-30 13:43:42] Decode batch. #running-req: 1, #token: 1865, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.56, #queue-req: 0, 
[2025-09-30 13:43:42] Decode batch. #running-req: 1, #token: 1905, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.44, #queue-req: 0, 
[2025-09-30 13:43:43] Decode batch. #running-req: 1, #token: 1945, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.52, #queue-req: 0, 
[2025-09-30 13:43:43] Decode batch. #running-req: 1, #token: 1985, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.45, #queue-req: 0, 
[2025-09-30 13:43:43] Decode batch. #running-req: 1, #token: 2025, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.47, #queue-req: 0, 
[2025-09-30 13:43:44] Decode batch. #running-req: 1, #token: 2065, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.18, #queue-req: 0, 
[2025-09-30 13:43:44] Decode batch. #running-req: 1, #token: 2105, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.92, #queue-req: 0, 
[2025-09-30 13:43:44] Decode batch. #running-req: 1, #token: 2145, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:43:45] Decode batch. #running-req: 1, #token: 2185, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.92, #queue-req: 0, 
[2025-09-30 13:43:45] Decode batch. #running-req: 1, #token: 2225, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.88, #queue-req: 0, 
[2025-09-30 13:43:45] Decode batch. #running-req: 1, #token: 2265, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.88, #queue-req: 0, 
[2025-09-30 13:43:45] Decode batch. #running-req: 1, #token: 2305, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:43:46] Decode batch. #running-req: 1, #token: 2345, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.37, #queue-req: 0, 
[2025-09-30 13:43:46] Decode batch. #running-req: 1, #token: 2385, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:43:46] Decode batch. #running-req: 1, #token: 2425, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:43:47] Decode batch. #running-req: 1, #token: 2465, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:43:47] Decode batch. #running-req: 1, #token: 2505, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.23, #queue-req: 0, 
[2025-09-30 13:43:47] Decode batch. #running-req: 1, #token: 2545, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.17, #queue-req: 0, 
[2025-09-30 13:43:48] Decode batch. #running-req: 1, #token: 2585, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.95, #queue-req: 0, 
[2025-09-30 13:43:48] Decode batch. #running-req: 1, #token: 2625, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.69, #queue-req: 0, 
[2025-09-30 13:43:48] Decode batch. #running-req: 1, #token: 2665, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.70, #queue-req: 0, 
[2025-09-30 13:43:49] Decode batch. #running-req: 1, #token: 2705, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.71, #queue-req: 0, 
[2025-09-30 13:43:49] Decode batch. #running-req: 1, #token: 2745, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.68, #queue-req: 0, 
[2025-09-30 13:43:49] Decode batch. #running-req: 1, #token: 2785, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.71, #queue-req: 0, 
[2025-09-30 13:43:50] Decode batch. #running-req: 1, #token: 2825, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.54, #queue-req: 0, 
[2025-09-30 13:43:50] Decode batch. #running-req: 1, #token: 2865, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.13, #queue-req: 0, 
[2025-09-30 13:43:50] Decode batch. #running-req: 1, #token: 2905, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.12, #queue-req: 0, 
[2025-09-30 13:43:51] Decode batch. #running-req: 1, #token: 2945, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.14, #queue-req: 0, 
[2025-09-30 13:43:51] Decode batch. #running-req: 1, #token: 2985, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.10, #queue-req: 0, 
[2025-09-30 13:43:51] Decode batch. #running-req: 1, #token: 3025, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.07, #queue-req: 0, 
[2025-09-30 13:43:51] Decode batch. #running-req: 1, #token: 3065, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.05, #queue-req: 0, 
[2025-09-30 13:43:52] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:52] Prefill batch. #new-seq: 1, #new-token: 475, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:52] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:52] Prefill batch. #new-seq: 1, #new-token: 249, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:52] Decode batch. #running-req: 1, #token: 439, token usage: 0.01, cuda graph: True, gen throughput (token/s): 49.74, #queue-req: 0, 
[2025-09-30 13:43:52] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:52] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:53] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.31, #queue-req: 0, 
[2025-09-30 13:43:53] Decode batch. #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.14, #queue-req: 0, 
[2025-09-30 13:43:53] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:53] Prefill batch. #new-seq: 1, #new-token: 532, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:54] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:54] Prefill batch. #new-seq: 1, #new-token: 290, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:54] Decode batch. #running-req: 1, #token: 474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 49.52, #queue-req: 0, 
[2025-09-30 13:43:54] Decode batch. #running-req: 1, #token: 514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.38, #queue-req: 0, 
[2025-09-30 13:43:54] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:54] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:54] Decode batch. #running-req: 1, #token: 242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.37, #queue-req: 0, 
[2025-09-30 13:43:55] Decode batch. #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.03, #queue-req: 0, 
[2025-09-30 13:43:55] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:55] Prefill batch. #new-seq: 1, #new-token: 566, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:55] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:55] Prefill batch. #new-seq: 1, #new-token: 284, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:55] Decode batch. #running-req: 1, #token: 481, token usage: 0.01, cuda graph: True, gen throughput (token/s): 49.05, #queue-req: 0, 
[2025-09-30 13:43:55] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:55] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:56] Decode batch. #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.03, #queue-req: 0, 
[2025-09-30 13:43:56] Decode batch. #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:43:56] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:57] Prefill batch. #new-seq: 1, #new-token: 653, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:57] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:57] Prefill batch. #new-seq: 1, #new-token: 376, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:57] Decode batch. #running-req: 1, #token: 563, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.29, #queue-req: 0, 
[2025-09-30 13:43:57] Decode batch. #running-req: 1, #token: 603, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.68, #queue-req: 0, 
[2025-09-30 13:43:57] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:57] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:58] Decode batch. #running-req: 1, #token: 255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.81, #queue-req: 0, 
[2025-09-30 13:43:58] Decode batch. #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.86, #queue-req: 0, 
[2025-09-30 13:43:58] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:58] Prefill batch. #new-seq: 1, #new-token: 987, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:59] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:59] Prefill batch. #new-seq: 1, #new-token: 615, #cached-token: 165, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:59] Decode batch. #running-req: 1, #token: 793, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.33, #queue-req: 0, 
[2025-09-30 13:43:59] Decode batch. #running-req: 1, #token: 833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.52, #queue-req: 0, 
[2025-09-30 13:43:59] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:43:59] Prefill batch. #new-seq: 1, #new-token: 66, #cached-token: 198, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:43:59] Decode batch. #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 122.91, #queue-req: 0, 
[2025-09-30 13:44:00] Decode batch. #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.91, #queue-req: 0, 
[2025-09-30 13:44:00] Decode batch. #running-req: 1, #token: 351, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.75, #queue-req: 0, 
[2025-09-30 13:44:00] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:00] Prefill batch. #new-seq: 1, #new-token: 1467, #cached-token: 80, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:01] Prefill batch. #new-seq: 1, #new-token: 856, #cached-token: 165, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:01] Decode batch. #running-req: 1, #token: 1047, token usage: 0.02, cuda graph: True, gen throughput (token/s): 45.27, #queue-req: 0, 
[2025-09-30 13:44:01] Decode batch. #running-req: 1, #token: 1087, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.87, #queue-req: 0, 
[2025-09-30 13:44:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:01] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 198, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:01] Decode batch. #running-req: 1, #token: 252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 122.53, #queue-req: 0, 
[2025-09-30 13:44:02] Decode batch. #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.93, #queue-req: 0, 
[2025-09-30 13:44:02] Decode batch. #running-req: 1, #token: 332, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.83, #queue-req: 0, 
[2025-09-30 13:44:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:03] Prefill batch. #new-seq: 1, #new-token: 1533, #cached-token: 80, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:03] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:03] Prefill batch. #new-seq: 1, #new-token: 683, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:03] Decode batch. #running-req: 1, #token: 873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.50, #queue-req: 0, 
[2025-09-30 13:44:03] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:03] Prefill batch. #new-seq: 1, #new-token: 74, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:03] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.24, #queue-req: 0, 
[2025-09-30 13:44:04] Decode batch. #running-req: 1, #token: 340, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.79, #queue-req: 0, 
[2025-09-30 13:44:04] Decode batch. #running-req: 1, #token: 380, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.78, #queue-req: 0, 
[2025-09-30 13:44:04] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:04] Prefill batch. #new-seq: 1, #new-token: 1416, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:04] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:04] Prefill batch. #new-seq: 1, #new-token: 738, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:04] Decode batch. #running-req: 1, #token: 936, token usage: 0.01, cuda graph: True, gen throughput (token/s): 95.25, #queue-req: 0, 
[2025-09-30 13:44:05] Decode batch. #running-req: 1, #token: 976, token usage: 0.02, cuda graph: True, gen throughput (token/s): 130.30, #queue-req: 0, 
[2025-09-30 13:44:05] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:05] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:05] Decode batch. #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.70, #queue-req: 0, 
[2025-09-30 13:44:05] Decode batch. #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.86, #queue-req: 0, 
[2025-09-30 13:44:05] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:06] Prefill batch. #new-seq: 1, #new-token: 1489, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:06] Prefill batch. #new-seq: 1, #new-token: 756, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:06] Decode batch. #running-req: 1, #token: 945, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.66, #queue-req: 0, 
[2025-09-30 13:44:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:06] Prefill batch. #new-seq: 1, #new-token: 88, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:06] Decode batch. #running-req: 1, #token: 296, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.51, #queue-req: 0, 
[2025-09-30 13:44:07] Decode batch. #running-req: 1, #token: 336, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:44:07] Decode batch. #running-req: 1, #token: 376, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.65, #queue-req: 0, 
[2025-09-30 13:44:07] Decode batch. #running-req: 1, #token: 416, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.56, #queue-req: 0, 
[2025-09-30 13:44:07] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:07] Prefill batch. #new-seq: 1, #new-token: 1184, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:07] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:07] Prefill batch. #new-seq: 1, #new-token: 435, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:08] Decode batch. #running-req: 1, #token: 624, token usage: 0.01, cuda graph: True, gen throughput (token/s): 102.24, #queue-req: 0, 
[2025-09-30 13:44:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:08] Prefill batch. #new-seq: 1, #new-token: 66, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:08] Decode batch. #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.54, #queue-req: 0, 
[2025-09-30 13:44:08] Decode batch. #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.91, #queue-req: 0, 
[2025-09-30 13:44:09] Decode batch. #running-req: 1, #token: 343, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.82, #queue-req: 0, 
[2025-09-30 13:44:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:09] Prefill batch. #new-seq: 1, #new-token: 980, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:09] Prefill batch. #new-seq: 1, #new-token: 548, #cached-token: 166, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:09] Decode batch. #running-req: 1, #token: 739, token usage: 0.01, cuda graph: True, gen throughput (token/s): 103.32, #queue-req: 0, 
[2025-09-30 13:44:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:09] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 199, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:09] Decode batch. #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.53, #queue-req: 0, 
[2025-09-30 13:44:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:10] Prefill batch. #new-seq: 1, #new-token: 1183, #cached-token: 81, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:10] Prefill batch. #new-seq: 1, #new-token: 640, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:10] Decode batch. #running-req: 1, #token: 807, token usage: 0.01, cuda graph: True, gen throughput (token/s): 46.71, #queue-req: 0, 
[2025-09-30 13:44:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:10] Prefill batch. #new-seq: 1, #new-token: 79, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:10] Decode batch. #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.68, #queue-req: 0, 
[2025-09-30 13:44:11] Decode batch. #running-req: 1, #token: 324, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.80, #queue-req: 0, 
[2025-09-30 13:44:11] Decode batch. #running-req: 1, #token: 364, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.68, #queue-req: 0, 
[2025-09-30 13:44:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:12] Prefill batch. #new-seq: 1, #new-token: 1083, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:12] Prefill batch. #new-seq: 1, #new-token: 450, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:12] Decode batch. #running-req: 1, #token: 642, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.10, #queue-req: 0, 
[2025-09-30 13:44:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:12] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:12] Decode batch. #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.40, #queue-req: 0, 
[2025-09-30 13:44:13] Decode batch. #running-req: 1, #token: 316, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.88, #queue-req: 0, 
[2025-09-30 13:44:13] Decode batch. #running-req: 1, #token: 356, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.71, #queue-req: 0, 
[2025-09-30 13:44:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:13] Prefill batch. #new-seq: 1, #new-token: 949, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:13] Prefill batch. #new-seq: 1, #new-token: 567, #cached-token: 153, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:14] Decode batch. #running-req: 1, #token: 750, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.10, #queue-req: 0, 
[2025-09-30 13:44:14] Decode batch. #running-req: 1, #token: 790, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.81, #queue-req: 0, 
[2025-09-30 13:44:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:14] Prefill batch. #new-seq: 1, #new-token: 63, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:14] Decode batch. #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.32, #queue-req: 0, 
[2025-09-30 13:44:15] Decode batch. #running-req: 1, #token: 309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.87, #queue-req: 0, 
[2025-09-30 13:44:15] Decode batch. #running-req: 1, #token: 349, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:44:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:15] Prefill batch. #new-seq: 1, #new-token: 649, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:16] Prefill batch. #new-seq: 1, #new-token: 147, #cached-token: 165, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:16] Decode batch. #running-req: 1, #token: 335, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-09-30 13:44:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:16] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 198, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:16] Decode batch. #running-req: 1, #token: 254, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.38, #queue-req: 0, 
[2025-09-30 13:44:16] Decode batch. #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.91, #queue-req: 0, 
[2025-09-30 13:44:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:17] Prefill batch. #new-seq: 1, #new-token: 458, #cached-token: 80, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:17] Prefill batch. #new-seq: 1, #new-token: 314, #cached-token: 166, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:17] Decode batch. #running-req: 1, #token: 487, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-09-30 13:44:17] Decode batch. #running-req: 1, #token: 527, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.15, #queue-req: 0, 
[2025-09-30 13:44:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:17] Prefill batch. #new-seq: 1, #new-token: 51, #cached-token: 199, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:18] Decode batch. #running-req: 1, #token: 285, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.42, #queue-req: 0, 
[2025-09-30 13:44:18] Decode batch. #running-req: 1, #token: 325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.77, #queue-req: 0, 
[2025-09-30 13:44:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:19] Prefill batch. #new-seq: 1, #new-token: 2033, #cached-token: 339, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:19] Decode batch. #running-req: 1, #token: 2394, token usage: 0.04, cuda graph: True, gen throughput (token/s): 46.16, #queue-req: 0, 
[2025-09-30 13:44:19] Decode batch. #running-req: 1, #token: 2434, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.20, #queue-req: 0, 
[2025-09-30 13:44:20] Decode batch. #running-req: 1, #token: 2474, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.18, #queue-req: 0, 
[2025-09-30 13:44:20] Decode batch. #running-req: 1, #token: 2514, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:44:20] Decode batch. #running-req: 1, #token: 2554, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.18, #queue-req: 0, 
[2025-09-30 13:44:21] Decode batch. #running-req: 1, #token: 2594, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.84, #queue-req: 0, 
[2025-09-30 13:44:21] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:21] Prefill batch. #new-seq: 1, #new-token: 2033, #cached-token: 319, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:21] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:21] Prefill batch. #new-seq: 1, #new-token: 1474, #cached-token: 229, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:21] Decode batch. #running-req: 1, #token: 1720, token usage: 0.03, cuda graph: True, gen throughput (token/s): 82.17, #queue-req: 0, 
[2025-09-30 13:44:21] Decode batch. #running-req: 1, #token: 1760, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.01, #queue-req: 0, 
[2025-09-30 13:44:22] Decode batch. #running-req: 1, #token: 1800, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.95, #queue-req: 0, 
[2025-09-30 13:44:22] Decode batch. #running-req: 1, #token: 1840, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.53, #queue-req: 0, 
[2025-09-30 13:44:22] Decode batch. #running-req: 1, #token: 1880, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.49, #queue-req: 0, 
[2025-09-30 13:44:23] Decode batch. #running-req: 1, #token: 1920, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.48, #queue-req: 0, 
[2025-09-30 13:44:23] Decode batch. #running-req: 1, #token: 1960, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.46, #queue-req: 0, 
[2025-09-30 13:44:23] Decode batch. #running-req: 1, #token: 2000, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.41, #queue-req: 0, 
[2025-09-30 13:44:23] Decode batch. #running-req: 1, #token: 2040, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.40, #queue-req: 0, 
[2025-09-30 13:44:24] Decode batch. #running-req: 1, #token: 2080, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.04, #queue-req: 0, 
[2025-09-30 13:44:24] Decode batch. #running-req: 1, #token: 2120, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.92, #queue-req: 0, 
[2025-09-30 13:44:24] Decode batch. #running-req: 1, #token: 2160, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.90, #queue-req: 0, 
[2025-09-30 13:44:25] Decode batch. #running-req: 1, #token: 2200, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.88, #queue-req: 0, 
[2025-09-30 13:44:25] Decode batch. #running-req: 1, #token: 2240, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.88, #queue-req: 0, 
[2025-09-30 13:44:25] Decode batch. #running-req: 1, #token: 2280, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.85, #queue-req: 0, 
[2025-09-30 13:44:26] Decode batch. #running-req: 1, #token: 2320, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.55, #queue-req: 0, 
[2025-09-30 13:44:26] Decode batch. #running-req: 1, #token: 2360, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.21, #queue-req: 0, 
[2025-09-30 13:44:26] Decode batch. #running-req: 1, #token: 2400, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.28, #queue-req: 0, 
[2025-09-30 13:44:27] Decode batch. #running-req: 1, #token: 2440, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.21, #queue-req: 0, 
[2025-09-30 13:44:27] Decode batch. #running-req: 1, #token: 2480, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.28, #queue-req: 0, 
[2025-09-30 13:44:27] Decode batch. #running-req: 1, #token: 2520, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.24, #queue-req: 0, 
[2025-09-30 13:44:28] Decode batch. #running-req: 1, #token: 2560, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.20, #queue-req: 0, 
[2025-09-30 13:44:28] Decode batch. #running-req: 1, #token: 2600, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.74, #queue-req: 0, 
[2025-09-30 13:44:28] Decode batch. #running-req: 1, #token: 2640, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.70, #queue-req: 0, 
[2025-09-30 13:44:29] Decode batch. #running-req: 1, #token: 2680, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.67, #queue-req: 0, 
[2025-09-30 13:44:29] Decode batch. #running-req: 1, #token: 2720, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.64, #queue-req: 0, 
[2025-09-30 13:44:29] Decode batch. #running-req: 1, #token: 2760, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.67, #queue-req: 0, 
[2025-09-30 13:44:29] Decode batch. #running-req: 1, #token: 2800, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.66, #queue-req: 0, 
[2025-09-30 13:44:30] Decode batch. #running-req: 1, #token: 2840, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.38, #queue-req: 0, 
[2025-09-30 13:44:30] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:31] Prefill batch. #new-seq: 1, #new-token: 624, #cached-token: 81, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:31] Prefill batch. #new-seq: 1, #new-token: 312, #cached-token: 167, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:31] Decode batch. #running-req: 1, #token: 487, token usage: 0.01, cuda graph: True, gen throughput (token/s): 43.23, #queue-req: 0, 
[2025-09-30 13:44:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:31] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:31] Decode batch. #running-req: 1, #token: 252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.70, #queue-req: 0, 
[2025-09-30 13:44:31] Decode batch. #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.93, #queue-req: 0, 
[2025-09-30 13:44:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:32] Prefill batch. #new-seq: 1, #new-token: 1087, #cached-token: 340, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:32] Decode batch. #running-req: 1, #token: 1446, token usage: 0.02, cuda graph: True, gen throughput (token/s): 47.76, #queue-req: 0, 
[2025-09-30 13:44:32] Decode batch. #running-req: 1, #token: 1486, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.72, #queue-req: 0, 
[2025-09-30 13:44:33] Decode batch. #running-req: 1, #token: 1526, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.59, #queue-req: 0, 
[2025-09-30 13:44:33] Decode batch. #running-req: 1, #token: 1566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.36, #queue-req: 0, 
[2025-09-30 13:44:33] Decode batch. #running-req: 1, #token: 1606, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.16, #queue-req: 0, 
[2025-09-30 13:44:34] Decode batch. #running-req: 1, #token: 1646, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.15, #queue-req: 0, 
[2025-09-30 13:44:34] Decode batch. #running-req: 1, #token: 1686, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.06, #queue-req: 0, 
[2025-09-30 13:44:34] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:34] Prefill batch. #new-seq: 1, #new-token: 1087, #cached-token: 320, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:34] Decode batch. #running-req: 1, #token: 1411, token usage: 0.02, cuda graph: True, gen throughput (token/s): 107.98, #queue-req: 0, 
[2025-09-30 13:44:35] Decode batch. #running-req: 1, #token: 1451, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.75, #queue-req: 0, 
[2025-09-30 13:44:35] Decode batch. #running-req: 1, #token: 1491, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.72, #queue-req: 0, 
[2025-09-30 13:44:35] Decode batch. #running-req: 1, #token: 1531, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.72, #queue-req: 0, 
[2025-09-30 13:44:36] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:36] Prefill batch. #new-seq: 1, #new-token: 1451, #cached-token: 125, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:36] Decode batch. #running-req: 1, #token: 1585, token usage: 0.02, cuda graph: True, gen throughput (token/s): 104.82, #queue-req: 0, 
[2025-09-30 13:44:36] Decode batch. #running-req: 1, #token: 1625, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.11, #queue-req: 0, 
[2025-09-30 13:44:36] Decode batch. #running-req: 1, #token: 1665, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.09, #queue-req: 0, 
[2025-09-30 13:44:37] Decode batch. #running-req: 1, #token: 1705, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.04, #queue-req: 0, 
[2025-09-30 13:44:37] Decode batch. #running-req: 1, #token: 1745, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.00, #queue-req: 0, 
[2025-09-30 13:44:37] Decode batch. #running-req: 1, #token: 1785, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.05, #queue-req: 0, 
[2025-09-30 13:44:38] Decode batch. #running-req: 1, #token: 1825, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.68, #queue-req: 0, 
[2025-09-30 13:44:38] Decode batch. #running-req: 1, #token: 1865, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.56, #queue-req: 0, 
[2025-09-30 13:44:38] Decode batch. #running-req: 1, #token: 1905, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.46, #queue-req: 0, 
[2025-09-30 13:44:38] Decode batch. #running-req: 1, #token: 1945, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.51, #queue-req: 0, 
[2025-09-30 13:44:39] Decode batch. #running-req: 1, #token: 1985, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.50, #queue-req: 0, 
[2025-09-30 13:44:39] Decode batch. #running-req: 1, #token: 2025, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.50, #queue-req: 0, 
[2025-09-30 13:44:39] Decode batch. #running-req: 1, #token: 2065, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.26, #queue-req: 0, 
[2025-09-30 13:44:40] Decode batch. #running-req: 1, #token: 2105, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.96, #queue-req: 0, 
[2025-09-30 13:44:40] Decode batch. #running-req: 1, #token: 2145, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.94, #queue-req: 0, 
[2025-09-30 13:44:40] Decode batch. #running-req: 1, #token: 2185, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.81, #queue-req: 0, 
[2025-09-30 13:44:41] Decode batch. #running-req: 1, #token: 2225, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.83, #queue-req: 0, 
[2025-09-30 13:44:41] Decode batch. #running-req: 1, #token: 2265, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.79, #queue-req: 0, 
[2025-09-30 13:44:41] Decode batch. #running-req: 1, #token: 2305, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.80, #queue-req: 0, 
[2025-09-30 13:44:42] Decode batch. #running-req: 1, #token: 2345, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.39, #queue-req: 0, 
[2025-09-30 13:44:42] Decode batch. #running-req: 1, #token: 2385, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.33, #queue-req: 0, 
[2025-09-30 13:44:42] Decode batch. #running-req: 1, #token: 2425, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.28, #queue-req: 0, 
[2025-09-30 13:44:43] Decode batch. #running-req: 1, #token: 2465, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.30, #queue-req: 0, 
[2025-09-30 13:44:43] Decode batch. #running-req: 1, #token: 2505, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.23, #queue-req: 0, 
[2025-09-30 13:44:43] Decode batch. #running-req: 1, #token: 2545, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.18, #queue-req: 0, 
[2025-09-30 13:44:43] Decode batch. #running-req: 1, #token: 2585, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.89, #queue-req: 0, 
[2025-09-30 13:44:44] Decode batch. #running-req: 1, #token: 2625, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.71, #queue-req: 0, 
[2025-09-30 13:44:44] Decode batch. #running-req: 1, #token: 2665, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.71, #queue-req: 0, 
[2025-09-30 13:44:44] Decode batch. #running-req: 1, #token: 2705, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.67, #queue-req: 0, 
[2025-09-30 13:44:45] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:45] Prefill batch. #new-seq: 1, #new-token: 730, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:45] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:45] Prefill batch. #new-seq: 1, #new-token: 424, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:45] Decode batch. #running-req: 1, #token: 598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 43.62, #queue-req: 0, 
[2025-09-30 13:44:46] Decode batch. #running-req: 1, #token: 638, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.64, #queue-req: 0, 
[2025-09-30 13:44:46] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:46] Prefill batch. #new-seq: 1, #new-token: 77, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:46] Decode batch. #running-req: 1, #token: 287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.15, #queue-req: 0, 
[2025-09-30 13:44:46] Decode batch. #running-req: 1, #token: 327, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.85, #queue-req: 0, 
[2025-09-30 13:44:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:47] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:47] Prefill batch. #new-seq: 1, #new-token: 422, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:47] Decode batch. #running-req: 1, #token: 497, token usage: 0.01, cuda graph: True, gen throughput (token/s): 66.99, #queue-req: 0, 
[2025-09-30 13:44:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:48] Prefill batch. #new-seq: 1, #new-token: 585, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:48] Prefill batch. #new-seq: 1, #new-token: 168, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:48] Decode batch. #running-req: 1, #token: 344, token usage: 0.01, cuda graph: True, gen throughput (token/s): 49.21, #queue-req: 0, 
[2025-09-30 13:44:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:48] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:48] Decode batch. #running-req: 1, #token: 262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.18, #queue-req: 0, 
[2025-09-30 13:44:48] Decode batch. #running-req: 1, #token: 302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.91, #queue-req: 0, 
[2025-09-30 13:44:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:49] Prefill batch. #new-seq: 1, #new-token: 458, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:49] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:49] Prefill batch. #new-seq: 1, #new-token: 297, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:49] Decode batch. #running-req: 1, #token: 483, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.33, #queue-req: 0, 
[2025-09-30 13:44:49] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:49] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:49] Decode batch. #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.58, #queue-req: 0, 
[2025-09-30 13:44:50] Decode batch. #running-req: 1, #token: 351, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.73, #queue-req: 0, 
[2025-09-30 13:44:50] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:50] Prefill batch. #new-seq: 1, #new-token: 549, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:51] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:51] Prefill batch. #new-seq: 1, #new-token: 258, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:51] Decode batch. #running-req: 1, #token: 425, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.91, #queue-req: 0, 
[2025-09-30 13:44:51] Decode batch. #running-req: 1, #token: 465, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.80, #queue-req: 0, 
[2025-09-30 13:44:51] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:51] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:51] Decode batch. #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.62, #queue-req: 0, 
[2025-09-30 13:44:52] Decode batch. #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.85, #queue-req: 0, 
[2025-09-30 13:44:52] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:52] Prefill batch. #new-seq: 1, #new-token: 654, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:52] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:52] Prefill batch. #new-seq: 1, #new-token: 402, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:52] Decode batch. #running-req: 1, #token: 596, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.24, #queue-req: 0, 
[2025-09-30 13:44:53] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:53] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:53] Decode batch. #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.77, #queue-req: 0, 
[2025-09-30 13:44:53] Decode batch. #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:44:53] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:53] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:54] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:54] Prefill batch. #new-seq: 1, #new-token: 398, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:54] Decode batch. #running-req: 1, #token: 473, token usage: 0.01, cuda graph: True, gen throughput (token/s): 67.12, #queue-req: 0, 
[2025-09-30 13:44:54] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:54] Prefill batch. #new-seq: 1, #new-token: 658, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:54] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:54] Prefill batch. #new-seq: 1, #new-token: 263, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:54] Decode batch. #running-req: 1, #token: 441, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.33, #queue-req: 0, 
[2025-09-30 13:44:55] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:55] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:55] Decode batch. #running-req: 1, #token: 231, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.38, #queue-req: 0, 
[2025-09-30 13:44:55] Decode batch. #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.07, #queue-req: 0, 
[2025-09-30 13:44:55] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:56] Prefill batch. #new-seq: 1, #new-token: 522, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:56] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:56] Prefill batch. #new-seq: 1, #new-token: 267, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:56] Decode batch. #running-req: 1, #token: 438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.58, #queue-req: 0, 
[2025-09-30 13:44:56] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:56] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:56] Decode batch. #running-req: 1, #token: 237, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.74, #queue-req: 0, 
[2025-09-30 13:44:56] Decode batch. #running-req: 1, #token: 277, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.99, #queue-req: 0, 
[2025-09-30 13:44:57] Decode batch. #running-req: 1, #token: 317, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.87, #queue-req: 0, 
[2025-09-30 13:44:57] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:44:58] Prefill batch. #new-seq: 1, #new-token: 2269, #cached-token: 341, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:44:58] Decode batch. #running-req: 1, #token: 2627, token usage: 0.04, cuda graph: True, gen throughput (token/s): 29.22, #queue-req: 0, 
[2025-09-30 13:44:58] Decode batch. #running-req: 1, #token: 2667, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.67, #queue-req: 0, 
[2025-09-30 13:44:59] Decode batch. #running-req: 1, #token: 2707, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.63, #queue-req: 0, 
[2025-09-30 13:44:59] Decode batch. #running-req: 1, #token: 2747, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.65, #queue-req: 0, 
[2025-09-30 13:44:59] Decode batch. #running-req: 1, #token: 2787, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.66, #queue-req: 0, 
[2025-09-30 13:45:00] Decode batch. #running-req: 1, #token: 2827, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.48, #queue-req: 0, 
[2025-09-30 13:45:00] Decode batch. #running-req: 1, #token: 2867, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.13, #queue-req: 0, 
[2025-09-30 13:45:00] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:00] Prefill batch. #new-seq: 1, #new-token: 2269, #cached-token: 321, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:00] Decode batch. #running-req: 1, #token: 2594, token usage: 0.04, cuda graph: True, gen throughput (token/s): 93.00, #queue-req: 0, 
[2025-09-30 13:45:01] Decode batch. #running-req: 1, #token: 2634, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.78, #queue-req: 0, 
[2025-09-30 13:45:01] Decode batch. #running-req: 1, #token: 2674, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.76, #queue-req: 0, 
[2025-09-30 13:45:01] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.60, #queue-req: 0, 
[2025-09-30 13:45:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:01] Prefill batch. #new-seq: 1, #new-token: 1203, #cached-token: 380, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:02] Decode batch. #running-req: 1, #token: 1623, token usage: 0.03, cuda graph: True, gen throughput (token/s): 107.55, #queue-req: 0, 
[2025-09-30 13:45:02] Decode batch. #running-req: 1, #token: 1663, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.08, #queue-req: 0, 
[2025-09-30 13:45:02] Decode batch. #running-req: 1, #token: 1703, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.08, #queue-req: 0, 
[2025-09-30 13:45:03] Decode batch. #running-req: 1, #token: 1743, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.00, #queue-req: 0, 
[2025-09-30 13:45:03] Decode batch. #running-req: 1, #token: 1783, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.02, #queue-req: 0, 
[2025-09-30 13:45:03] Decode batch. #running-req: 1, #token: 1823, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.71, #queue-req: 0, 
[2025-09-30 13:45:04] Decode batch. #running-req: 1, #token: 1863, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.52, #queue-req: 0, 
[2025-09-30 13:45:04] Decode batch. #running-req: 1, #token: 1903, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.46, #queue-req: 0, 
[2025-09-30 13:45:04] Decode batch. #running-req: 1, #token: 1943, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.47, #queue-req: 0, 
[2025-09-30 13:45:05] Decode batch. #running-req: 1, #token: 1983, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.47, #queue-req: 0, 
[2025-09-30 13:45:05] Decode batch. #running-req: 1, #token: 2023, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.46, #queue-req: 0, 
[2025-09-30 13:45:05] Decode batch. #running-req: 1, #token: 2063, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.29, #queue-req: 0, 
[2025-09-30 13:45:06] Decode batch. #running-req: 1, #token: 2103, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.97, #queue-req: 0, 
[2025-09-30 13:45:06] Decode batch. #running-req: 1, #token: 2143, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.97, #queue-req: 0, 
[2025-09-30 13:45:06] Decode batch. #running-req: 1, #token: 2183, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:45:06] Decode batch. #running-req: 1, #token: 2223, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.80, #queue-req: 0, 
[2025-09-30 13:45:07] Decode batch. #running-req: 1, #token: 2263, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:45:07] Decode batch. #running-req: 1, #token: 2303, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.78, #queue-req: 0, 
[2025-09-30 13:45:07] Decode batch. #running-req: 1, #token: 2343, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.34, #queue-req: 0, 
[2025-09-30 13:45:08] Decode batch. #running-req: 1, #token: 2383, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.29, #queue-req: 0, 
[2025-09-30 13:45:08] Decode batch. #running-req: 1, #token: 2423, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:45:08] Decode batch. #running-req: 1, #token: 2463, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.25, #queue-req: 0, 
[2025-09-30 13:45:09] Decode batch. #running-req: 1, #token: 2503, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.24, #queue-req: 0, 
[2025-09-30 13:45:09] Decode batch. #running-req: 1, #token: 2543, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.17, #queue-req: 0, 
[2025-09-30 13:45:09] Decode batch. #running-req: 1, #token: 2583, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.91, #queue-req: 0, 
[2025-09-30 13:45:10] Decode batch. #running-req: 1, #token: 2623, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.76, #queue-req: 0, 
[2025-09-30 13:45:10] Decode batch. #running-req: 1, #token: 2663, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.75, #queue-req: 0, 
[2025-09-30 13:45:10] Decode batch. #running-req: 1, #token: 2703, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.73, #queue-req: 0, 
[2025-09-30 13:45:11] Decode batch. #running-req: 1, #token: 2743, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.71, #queue-req: 0, 
[2025-09-30 13:45:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:11] Prefill batch. #new-seq: 1, #new-token: 544, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:11] Prefill batch. #new-seq: 1, #new-token: 331, #cached-token: 153, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:12] Decode batch. #running-req: 1, #token: 492, token usage: 0.01, cuda graph: True, gen throughput (token/s): 41.48, #queue-req: 0, 
[2025-09-30 13:45:12] Decode batch. #running-req: 1, #token: 532, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.22, #queue-req: 0, 
[2025-09-30 13:45:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:12] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:12] Decode batch. #running-req: 1, #token: 236, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.37, #queue-req: 0, 
[2025-09-30 13:45:12] Decode batch. #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.05, #queue-req: 0, 
[2025-09-30 13:45:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:13] Prefill batch. #new-seq: 1, #new-token: 641, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:13] Prefill batch. #new-seq: 1, #new-token: 363, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:13] Decode batch. #running-req: 1, #token: 536, token usage: 0.01, cuda graph: True, gen throughput (token/s): 46.92, #queue-req: 0, 
[2025-09-30 13:45:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:13] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:14] Decode batch. #running-req: 1, #token: 254, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.39, #queue-req: 0, 
[2025-09-30 13:45:14] Decode batch. #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.87, #queue-req: 0, 
[2025-09-30 13:45:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:14] Prefill batch. #new-seq: 1, #new-token: 479, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:14] Prefill batch. #new-seq: 1, #new-token: 155, #cached-token: 153, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:15] Decode batch. #running-req: 1, #token: 339, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.12, #queue-req: 0, 
[2025-09-30 13:45:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:15] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:15] Decode batch. #running-req: 1, #token: 245, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.31, #queue-req: 0, 
[2025-09-30 13:45:15] Decode batch. #running-req: 1, #token: 285, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.97, #queue-req: 0, 
[2025-09-30 13:45:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:16] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 69, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:16] Decode batch. #running-req: 1, #token: 93, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.27, #queue-req: 0, 
[2025-09-30 13:45:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:16] Prefill batch. #new-seq: 1, #new-token: 120, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:16] Decode batch. #running-req: 1, #token: 217, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.15, #queue-req: 0, 
[2025-09-30 13:45:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:17] Prefill batch. #new-seq: 1, #new-token: 411, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:17] Prefill batch. #new-seq: 1, #new-token: 293, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:17] Decode batch. #running-req: 1, #token: 492, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.58, #queue-req: 0, 
[2025-09-30 13:45:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:17] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:17] Decode batch. #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.91, #queue-req: 0, 
[2025-09-30 13:45:18] Decode batch. #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.97, #queue-req: 0, 
[2025-09-30 13:45:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:18] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:18] Prefill batch. #new-seq: 1, #new-token: 289, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:18] Decode batch. #running-req: 1, #token: 372, token usage: 0.01, cuda graph: True, gen throughput (token/s): 66.94, #queue-req: 0, 
[2025-09-30 13:45:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:19] Prefill batch. #new-seq: 1, #new-token: 303, #cached-token: 341, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:19] Decode batch. #running-req: 1, #token: 672, token usage: 0.01, cuda graph: True, gen throughput (token/s): 50.20, #queue-req: 0, 
[2025-09-30 13:45:19] Decode batch. #running-req: 1, #token: 712, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.88, #queue-req: 0, 
[2025-09-30 13:45:20] Decode batch. #running-req: 1, #token: 752, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.83, #queue-req: 0, 
[2025-09-30 13:45:20] Decode batch. #running-req: 1, #token: 792, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.55, #queue-req: 0, 
[2025-09-30 13:45:20] Decode batch. #running-req: 1, #token: 832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.36, #queue-req: 0, 
[2025-09-30 13:45:21] Decode batch. #running-req: 1, #token: 872, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.29, #queue-req: 0, 
[2025-09-30 13:45:21] Decode batch. #running-req: 1, #token: 912, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.20, #queue-req: 0, 
[2025-09-30 13:45:21] Decode batch. #running-req: 1, #token: 952, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.02, #queue-req: 0, 
[2025-09-30 13:45:22] Decode batch. #running-req: 1, #token: 992, token usage: 0.02, cuda graph: True, gen throughput (token/s): 130.94, #queue-req: 0, 
[2025-09-30 13:45:22] Decode batch. #running-req: 1, #token: 1032, token usage: 0.02, cuda graph: True, gen throughput (token/s): 130.96, #queue-req: 0, 
[2025-09-30 13:45:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:22] Prefill batch. #new-seq: 1, #new-token: 303, #cached-token: 321, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:22] Decode batch. #running-req: 1, #token: 632, token usage: 0.01, cuda graph: True, gen throughput (token/s): 121.28, #queue-req: 0, 
[2025-09-30 13:45:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:22] Prefill batch. #new-seq: 1, #new-token: 1439, #cached-token: 288, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:23] Decode batch. #running-req: 1, #token: 1742, token usage: 0.03, cuda graph: True, gen throughput (token/s): 105.80, #queue-req: 0, 
[2025-09-30 13:45:23] Decode batch. #running-req: 1, #token: 1782, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.02, #queue-req: 0, 
[2025-09-30 13:45:23] Decode batch. #running-req: 1, #token: 1822, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.67, #queue-req: 0, 
[2025-09-30 13:45:23] Decode batch. #running-req: 1, #token: 1862, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.51, #queue-req: 0, 
[2025-09-30 13:45:24] Decode batch. #running-req: 1, #token: 1902, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:45:24] Decode batch. #running-req: 1, #token: 1942, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.39, #queue-req: 0, 
[2025-09-30 13:45:24] Decode batch. #running-req: 1, #token: 1982, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:45:25] Decode batch. #running-req: 1, #token: 2022, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.41, #queue-req: 0, 
[2025-09-30 13:45:25] Decode batch. #running-req: 1, #token: 2062, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.24, #queue-req: 0, 
[2025-09-30 13:45:25] Decode batch. #running-req: 1, #token: 2102, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:45:26] Decode batch. #running-req: 1, #token: 2142, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.85, #queue-req: 0, 
[2025-09-30 13:45:26] Decode batch. #running-req: 1, #token: 2182, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.86, #queue-req: 0, 
[2025-09-30 13:45:26] Decode batch. #running-req: 1, #token: 2222, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.83, #queue-req: 0, 
[2025-09-30 13:45:27] Decode batch. #running-req: 1, #token: 2262, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.83, #queue-req: 0, 
[2025-09-30 13:45:27] Decode batch. #running-req: 1, #token: 2302, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.82, #queue-req: 0, 
[2025-09-30 13:45:27] Decode batch. #running-req: 1, #token: 2342, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.35, #queue-req: 0, 
[2025-09-30 13:45:28] Decode batch. #running-req: 1, #token: 2382, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.27, #queue-req: 0, 
[2025-09-30 13:45:28] Decode batch. #running-req: 1, #token: 2422, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.16, #queue-req: 0, 
[2025-09-30 13:45:28] Decode batch. #running-req: 1, #token: 2462, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.26, #queue-req: 0, 
[2025-09-30 13:45:28] Decode batch. #running-req: 1, #token: 2502, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.19, #queue-req: 0, 
[2025-09-30 13:45:29] Decode batch. #running-req: 1, #token: 2542, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.16, #queue-req: 0, 
[2025-09-30 13:45:29] Decode batch. #running-req: 1, #token: 2582, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.91, #queue-req: 0, 
[2025-09-30 13:45:29] Decode batch. #running-req: 1, #token: 2622, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.67, #queue-req: 0, 
[2025-09-30 13:45:30] Decode batch. #running-req: 1, #token: 2662, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.64, #queue-req: 0, 
[2025-09-30 13:45:30] Decode batch. #running-req: 1, #token: 2702, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.64, #queue-req: 0, 
[2025-09-30 13:45:30] Decode batch. #running-req: 1, #token: 2742, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.59, #queue-req: 0, 
[2025-09-30 13:45:31] Decode batch. #running-req: 1, #token: 2782, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.58, #queue-req: 0, 
[2025-09-30 13:45:31] Decode batch. #running-req: 1, #token: 2822, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.51, #queue-req: 0, 
[2025-09-30 13:45:31] Decode batch. #running-req: 1, #token: 2862, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.12, #queue-req: 0, 
[2025-09-30 13:45:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:32] Prefill batch. #new-seq: 1, #new-token: 568, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:32] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:32] Prefill batch. #new-seq: 1, #new-token: 280, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:32] Decode batch. #running-req: 1, #token: 471, token usage: 0.01, cuda graph: True, gen throughput (token/s): 43.62, #queue-req: 0, 
[2025-09-30 13:45:32] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:32] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:33] Decode batch. #running-req: 1, #token: 229, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:45:33] Decode batch. #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.11, #queue-req: 0, 
[2025-09-30 13:45:33] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:34] Prefill batch. #new-seq: 1, #new-token: 3498, #cached-token: 341, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:34] Decode batch. #running-req: 1, #token: 3851, token usage: 0.06, cuda graph: True, gen throughput (token/s): 41.23, #queue-req: 0, 
[2025-09-30 13:45:34] Decode batch. #running-req: 1, #token: 3891, token usage: 0.06, cuda graph: True, gen throughput (token/s): 123.64, #queue-req: 0, 
[2025-09-30 13:45:34] Decode batch. #running-req: 1, #token: 3931, token usage: 0.06, cuda graph: True, gen throughput (token/s): 123.62, #queue-req: 0, 
[2025-09-30 13:45:35] Decode batch. #running-req: 1, #token: 3971, token usage: 0.06, cuda graph: True, gen throughput (token/s): 123.62, #queue-req: 0, 
[2025-09-30 13:45:35] Decode batch. #running-req: 1, #token: 4011, token usage: 0.06, cuda graph: True, gen throughput (token/s): 123.56, #queue-req: 0, 
[2025-09-30 13:45:35] Decode batch. #running-req: 1, #token: 4051, token usage: 0.06, cuda graph: True, gen throughput (token/s): 123.59, #queue-req: 0, 
[2025-09-30 13:45:36] Decode batch. #running-req: 1, #token: 4091, token usage: 0.06, cuda graph: True, gen throughput (token/s): 123.67, #queue-req: 0, 
[2025-09-30 13:45:36] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:36] Prefill batch. #new-seq: 1, #new-token: 3498, #cached-token: 321, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:36] Decode batch. #running-req: 1, #token: 3845, token usage: 0.06, cuda graph: True, gen throughput (token/s): 80.42, #queue-req: 0, 
[2025-09-30 13:45:37] Decode batch. #running-req: 1, #token: 3885, token usage: 0.06, cuda graph: True, gen throughput (token/s): 123.82, #queue-req: 0, 
[2025-09-30 13:45:37] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:37] Prefill batch. #new-seq: 1, #new-token: 1334, #cached-token: 203, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:37] Decode batch. #running-req: 1, #token: 1549, token usage: 0.02, cuda graph: True, gen throughput (token/s): 104.04, #queue-req: 0, 
[2025-09-30 13:45:37] Decode batch. #running-req: 1, #token: 1589, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.14, #queue-req: 0, 
[2025-09-30 13:45:38] Decode batch. #running-req: 1, #token: 1629, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.07, #queue-req: 0, 
[2025-09-30 13:45:38] Decode batch. #running-req: 1, #token: 1669, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.04, #queue-req: 0, 
[2025-09-30 13:45:38] Decode batch. #running-req: 1, #token: 1709, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.98, #queue-req: 0, 
[2025-09-30 13:45:39] Decode batch. #running-req: 1, #token: 1749, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.98, #queue-req: 0, 
[2025-09-30 13:45:39] Decode batch. #running-req: 1, #token: 1789, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.99, #queue-req: 0, 
[2025-09-30 13:45:39] Decode batch. #running-req: 1, #token: 1829, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.57, #queue-req: 0, 
[2025-09-30 13:45:39] Decode batch. #running-req: 1, #token: 1869, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.44, #queue-req: 0, 
[2025-09-30 13:45:40] Decode batch. #running-req: 1, #token: 1909, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.40, #queue-req: 0, 
[2025-09-30 13:45:40] Decode batch. #running-req: 1, #token: 1949, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.42, #queue-req: 0, 
[2025-09-30 13:45:40] Decode batch. #running-req: 1, #token: 1989, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.44, #queue-req: 0, 
[2025-09-30 13:45:41] Decode batch. #running-req: 1, #token: 2029, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.42, #queue-req: 0, 
[2025-09-30 13:45:41] Decode batch. #running-req: 1, #token: 2069, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.18, #queue-req: 0, 
[2025-09-30 13:45:41] Decode batch. #running-req: 1, #token: 2109, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.92, #queue-req: 0, 
[2025-09-30 13:45:42] Decode batch. #running-req: 1, #token: 2149, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.91, #queue-req: 0, 
[2025-09-30 13:45:42] Decode batch. #running-req: 1, #token: 2189, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.78, #queue-req: 0, 
[2025-09-30 13:45:42] Decode batch. #running-req: 1, #token: 2229, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.74, #queue-req: 0, 
[2025-09-30 13:45:43] Decode batch. #running-req: 1, #token: 2269, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.83, #queue-req: 0, 
[2025-09-30 13:45:43] Decode batch. #running-req: 1, #token: 2309, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.72, #queue-req: 0, 
[2025-09-30 13:45:43] Decode batch. #running-req: 1, #token: 2349, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.28, #queue-req: 0, 
[2025-09-30 13:45:44] Decode batch. #running-req: 1, #token: 2389, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.25, #queue-req: 0, 
[2025-09-30 13:45:44] Decode batch. #running-req: 1, #token: 2429, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.24, #queue-req: 0, 
[2025-09-30 13:45:44] Decode batch. #running-req: 1, #token: 2469, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.19, #queue-req: 0, 
[2025-09-30 13:45:44] Decode batch. #running-req: 1, #token: 2509, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.17, #queue-req: 0, 
[2025-09-30 13:45:45] Decode batch. #running-req: 1, #token: 2549, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.16, #queue-req: 0, 
[2025-09-30 13:45:45] Decode batch. #running-req: 1, #token: 2589, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.87, #queue-req: 0, 
[2025-09-30 13:45:45] Decode batch. #running-req: 1, #token: 2629, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.70, #queue-req: 0, 
[2025-09-30 13:45:46] Decode batch. #running-req: 1, #token: 2669, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.69, #queue-req: 0, 
[2025-09-30 13:45:46] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:46] Prefill batch. #new-seq: 1, #new-token: 410, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:46] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:46] Prefill batch. #new-seq: 1, #new-token: 136, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:47] Decode batch. #running-req: 1, #token: 331, token usage: 0.01, cuda graph: True, gen throughput (token/s): 42.60, #queue-req: 0, 
[2025-09-30 13:45:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:47] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:47] Decode batch. #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.45, #queue-req: 0, 
[2025-09-30 13:45:47] Decode batch. #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.88, #queue-req: 0, 
[2025-09-30 13:45:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:48] Prefill batch. #new-seq: 1, #new-token: 390, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:48] Prefill batch. #new-seq: 1, #new-token: 262, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:48] Decode batch. #running-req: 1, #token: 456, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.39, #queue-req: 0, 
[2025-09-30 13:45:48] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:48] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:48] Decode batch. #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.92, #queue-req: 0, 
[2025-09-30 13:45:49] Decode batch. #running-req: 1, #token: 306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.85, #queue-req: 0, 
[2025-09-30 13:45:49] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:49] Prefill batch. #new-seq: 1, #new-token: 1412, #cached-token: 339, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:50] Decode batch. #running-req: 1, #token: 1776, token usage: 0.03, cuda graph: True, gen throughput (token/s): 46.05, #queue-req: 0, 
[2025-09-30 13:45:50] Decode batch. #running-req: 1, #token: 1816, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.72, #queue-req: 0, 
[2025-09-30 13:45:50] Decode batch. #running-req: 1, #token: 1856, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.53, #queue-req: 0, 
[2025-09-30 13:45:51] Decode batch. #running-req: 1, #token: 1896, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.52, #queue-req: 0, 
[2025-09-30 13:45:51] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:51] Prefill batch. #new-seq: 1, #new-token: 1412, #cached-token: 319, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:51] Decode batch. #running-req: 1, #token: 1756, token usage: 0.03, cuda graph: True, gen throughput (token/s): 103.74, #queue-req: 0, 
[2025-09-30 13:45:51] Decode batch. #running-req: 1, #token: 1796, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.95, #queue-req: 0, 
[2025-09-30 13:45:51] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:51] Prefill batch. #new-seq: 1, #new-token: 1319, #cached-token: 129, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:52] Decode batch. #running-req: 1, #token: 1481, token usage: 0.02, cuda graph: True, gen throughput (token/s): 106.38, #queue-req: 0, 
[2025-09-30 13:45:52] Decode batch. #running-req: 1, #token: 1521, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.54, #queue-req: 0, 
[2025-09-30 13:45:52] Decode batch. #running-req: 1, #token: 1561, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.29, #queue-req: 0, 
[2025-09-30 13:45:53] Decode batch. #running-req: 1, #token: 1601, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.06, #queue-req: 0, 
[2025-09-30 13:45:53] Decode batch. #running-req: 1, #token: 1641, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.97, #queue-req: 0, 
[2025-09-30 13:45:53] Decode batch. #running-req: 1, #token: 1681, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.99, #queue-req: 0, 
[2025-09-30 13:45:53] Decode batch. #running-req: 1, #token: 1721, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.97, #queue-req: 0, 
[2025-09-30 13:45:54] Decode batch. #running-req: 1, #token: 1761, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.95, #queue-req: 0, 
[2025-09-30 13:45:54] Decode batch. #running-req: 1, #token: 1801, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.80, #queue-req: 0, 
[2025-09-30 13:45:54] Decode batch. #running-req: 1, #token: 1841, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.45, #queue-req: 0, 
[2025-09-30 13:45:55] Decode batch. #running-req: 1, #token: 1881, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.44, #queue-req: 0, 
[2025-09-30 13:45:55] Decode batch. #running-req: 1, #token: 1921, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.42, #queue-req: 0, 
[2025-09-30 13:45:55] Decode batch. #running-req: 1, #token: 1961, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.36, #queue-req: 0, 
[2025-09-30 13:45:56] Decode batch. #running-req: 1, #token: 2001, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.39, #queue-req: 0, 
[2025-09-30 13:45:56] Decode batch. #running-req: 1, #token: 2041, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.35, #queue-req: 0, 
[2025-09-30 13:45:56] Decode batch. #running-req: 1, #token: 2081, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.97, #queue-req: 0, 
[2025-09-30 13:45:57] Decode batch. #running-req: 1, #token: 2121, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.69, #queue-req: 0, 
[2025-09-30 13:45:57] Decode batch. #running-req: 1, #token: 2161, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.79, #queue-req: 0, 
[2025-09-30 13:45:57] Decode batch. #running-req: 1, #token: 2201, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.74, #queue-req: 0, 
[2025-09-30 13:45:58] Decode batch. #running-req: 1, #token: 2241, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.74, #queue-req: 0, 
[2025-09-30 13:45:58] Decode batch. #running-req: 1, #token: 2281, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.71, #queue-req: 0, 
[2025-09-30 13:45:58] Decode batch. #running-req: 1, #token: 2321, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.52, #queue-req: 0, 
[2025-09-30 13:45:58] Decode batch. #running-req: 1, #token: 2361, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:45:59] Decode batch. #running-req: 1, #token: 2401, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.19, #queue-req: 0, 
[2025-09-30 13:45:59] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:59] Prefill batch. #new-seq: 1, #new-token: 526, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:45:59] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:45:59] Prefill batch. #new-seq: 1, #new-token: 272, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:00] Decode batch. #running-req: 1, #token: 457, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.81, #queue-req: 0, 
[2025-09-30 13:46:00] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:00] Prefill batch. #new-seq: 1, #new-token: 51, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:00] Decode batch. #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.71, #queue-req: 0, 
[2025-09-30 13:46:00] Decode batch. #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:46:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:01] Prefill batch. #new-seq: 1, #new-token: 501, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:01] Prefill batch. #new-seq: 1, #new-token: 286, #cached-token: 153, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:01] Decode batch. #running-req: 1, #token: 442, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.37, #queue-req: 0, 
[2025-09-30 13:46:01] Decode batch. #running-req: 1, #token: 482, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.41, #queue-req: 0, 
[2025-09-30 13:46:02] Decode batch. #running-req: 1, #token: 522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.42, #queue-req: 0, 
[2025-09-30 13:46:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:02] Prefill batch. #new-seq: 1, #new-token: 39, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:02] Decode batch. #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.18, #queue-req: 0, 
[2025-09-30 13:46:02] Decode batch. #running-req: 1, #token: 309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.82, #queue-req: 0, 
[2025-09-30 13:46:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:02] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:03] Decode batch. #running-req: 1, #token: 111, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.65, #queue-req: 0, 
[2025-09-30 13:46:03] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:03] Prefill batch. #new-seq: 1, #new-token: 233, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:03] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:04] Prefill batch. #new-seq: 1, #new-token: 743, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:04] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:04] Prefill batch. #new-seq: 1, #new-token: 510, #cached-token: 166, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:04] Decode batch. #running-req: 1, #token: 690, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.52, #queue-req: 0, 
[2025-09-30 13:46:04] Decode batch. #running-req: 1, #token: 730, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.16, #queue-req: 0, 
[2025-09-30 13:46:04] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:04] Prefill batch. #new-seq: 1, #new-token: 52, #cached-token: 199, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:04] Decode batch. #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.61, #queue-req: 0, 
[2025-09-30 13:46:05] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:05] Prefill batch. #new-seq: 1, #new-token: 694, #cached-token: 81, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:05] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:05] Prefill batch. #new-seq: 1, #new-token: 192, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:05] Decode batch. #running-req: 1, #token: 375, token usage: 0.01, cuda graph: True, gen throughput (token/s): 65.55, #queue-req: 0, 
[2025-09-30 13:46:05] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:05] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:05] Decode batch. #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.59, #queue-req: 0, 
[2025-09-30 13:46:06] Decode batch. #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.86, #queue-req: 0, 
[2025-09-30 13:46:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:06] Prefill batch. #new-seq: 1, #new-token: 533, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:06] Prefill batch. #new-seq: 1, #new-token: 347, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:07] Decode batch. #running-req: 1, #token: 526, token usage: 0.01, cuda graph: True, gen throughput (token/s): 46.14, #queue-req: 0, 
[2025-09-30 13:46:07] Decode batch. #running-req: 1, #token: 566, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.96, #queue-req: 0, 
[2025-09-30 13:46:07] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:07] Prefill batch. #new-seq: 1, #new-token: 69, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:07] Decode batch. #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.29, #queue-req: 0, 
[2025-09-30 13:46:07] Decode batch. #running-req: 1, #token: 329, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.83, #queue-req: 0, 
[2025-09-30 13:46:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:08] Prefill batch. #new-seq: 1, #new-token: 643, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:08] Prefill batch. #new-seq: 1, #new-token: 303, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:08] Decode batch. #running-req: 1, #token: 484, token usage: 0.01, cuda graph: True, gen throughput (token/s): 46.30, #queue-req: 0, 
[2025-09-30 13:46:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:08] Prefill batch. #new-seq: 1, #new-token: 59, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:09] Decode batch. #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.42, #queue-req: 0, 
[2025-09-30 13:46:09] Decode batch. #running-req: 1, #token: 309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.88, #queue-req: 0, 
[2025-09-30 13:46:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:09] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.74, #queue-req: 0, 
[2025-09-30 13:46:09] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:09] Prefill batch. #new-seq: 1, #new-token: 299, #cached-token: 72, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:10] Prefill batch. #new-seq: 1, #new-token: 597, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:10] Prefill batch. #new-seq: 1, #new-token: 301, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:10] Decode batch. #running-req: 1, #token: 474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 34.75, #queue-req: 0, 
[2025-09-30 13:46:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:11] Prefill batch. #new-seq: 1, #new-token: 75, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:11] Decode batch. #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.01, #queue-req: 0, 
[2025-09-30 13:46:11] Decode batch. #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.82, #queue-req: 0, 
[2025-09-30 13:46:11] Decode batch. #running-req: 1, #token: 354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.74, #queue-req: 0, 
[2025-09-30 13:46:11] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:11] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:12] Prefill batch. #new-seq: 1, #new-token: 297, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:12] Decode batch. #running-req: 1, #token: 378, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.37, #queue-req: 0, 
[2025-09-30 13:46:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:12] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:12] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 369, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:12] Decode batch. #running-req: 1, #token: 383, token usage: 0.01, cuda graph: True, gen throughput (token/s): 48.40, #queue-req: 0, 
[2025-09-30 13:46:12] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:13] Prefill batch. #new-seq: 1, #new-token: 683, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:13] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:13] Prefill batch. #new-seq: 1, #new-token: 388, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:13] Decode batch. #running-req: 1, #token: 585, token usage: 0.01, cuda graph: True, gen throughput (token/s): 46.28, #queue-req: 0, 
[2025-09-30 13:46:14] Decode batch. #running-req: 1, #token: 625, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.58, #queue-req: 0, 
[2025-09-30 13:46:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:14] Prefill batch. #new-seq: 1, #new-token: 198, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:14] Decode batch. #running-req: 1, #token: 431, token usage: 0.01, cuda graph: True, gen throughput (token/s): 125.26, #queue-req: 0, 
[2025-09-30 13:46:14] Decode batch. #running-req: 1, #token: 471, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.30, #queue-req: 0, 
[2025-09-30 13:46:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:14] Prefill batch. #new-seq: 1, #new-token: 1075, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:14] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:14] Prefill batch. #new-seq: 1, #new-token: 690, #cached-token: 166, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:15] Decode batch. #running-req: 1, #token: 879, token usage: 0.01, cuda graph: True, gen throughput (token/s): 99.29, #queue-req: 0, 
[2025-09-30 13:46:15] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:15] Prefill batch. #new-seq: 1, #new-token: 73, #cached-token: 199, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:15] Decode batch. #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.71, #queue-req: 0, 
[2025-09-30 13:46:15] Decode batch. #running-req: 1, #token: 330, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.86, #queue-req: 0, 
[2025-09-30 13:46:16] Decode batch. #running-req: 1, #token: 370, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.77, #queue-req: 0, 
[2025-09-30 13:46:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:16] Prefill batch. #new-seq: 1, #new-token: 1271, #cached-token: 81, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:16] Prefill batch. #new-seq: 1, #new-token: 587, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:16] Decode batch. #running-req: 1, #token: 759, token usage: 0.01, cuda graph: True, gen throughput (token/s): 99.74, #queue-req: 0, 
[2025-09-30 13:46:16] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:16] Prefill batch. #new-seq: 1, #new-token: 50, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:16] Decode batch. #running-req: 1, #token: 256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.99, #queue-req: 0, 
[2025-09-30 13:46:17] Decode batch. #running-req: 1, #token: 296, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.89, #queue-req: 0, 
[2025-09-30 13:46:17] Decode batch. #running-req: 1, #token: 336, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.87, #queue-req: 0, 
[2025-09-30 13:46:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:17] Prefill batch. #new-seq: 1, #new-token: 1096, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:17] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:17] Prefill batch. #new-seq: 1, #new-token: 514, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:17] Decode batch. #running-req: 1, #token: 702, token usage: 0.01, cuda graph: True, gen throughput (token/s): 100.86, #queue-req: 0, 
[2025-09-30 13:46:18] Decode batch. #running-req: 1, #token: 742, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.14, #queue-req: 0, 
[2025-09-30 13:46:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:18] Prefill batch. #new-seq: 1, #new-token: 68, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:18] Decode batch. #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.33, #queue-req: 0, 
[2025-09-30 13:46:18] Decode batch. #running-req: 1, #token: 339, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.80, #queue-req: 0, 
[2025-09-30 13:46:18] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:19] Prefill batch. #new-seq: 1, #new-token: 1034, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:19] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:19] Prefill batch. #new-seq: 1, #new-token: 521, #cached-token: 168, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:19] Decode batch. #running-req: 1, #token: 709, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-09-30 13:46:19] Decode batch. #running-req: 1, #token: 749, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.14, #queue-req: 0, 
[2025-09-30 13:46:19] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:19] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 201, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:20] Decode batch. #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.47, #queue-req: 0, 
[2025-09-30 13:46:20] Decode batch. #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.83, #queue-req: 0, 
[2025-09-30 13:46:20] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:21] Prefill batch. #new-seq: 1, #new-token: 1144, #cached-token: 83, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:21] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:21] Prefill batch. #new-seq: 1, #new-token: 630, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:21] Decode batch. #running-req: 1, #token: 814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.10, #queue-req: 0, 
[2025-09-30 13:46:21] Decode batch. #running-req: 1, #token: 854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.50, #queue-req: 0, 
[2025-09-30 13:46:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:22] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:22] Decode batch. #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.06, #queue-req: 0, 
[2025-09-30 13:46:22] Decode batch. #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.92, #queue-req: 0, 
[2025-09-30 13:46:22] Decode batch. #running-req: 1, #token: 344, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.73, #queue-req: 0, 
[2025-09-30 13:46:22] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:23] Prefill batch. #new-seq: 1, #new-token: 1178, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:23] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:23] Prefill batch. #new-seq: 1, #new-token: 554, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:23] Decode batch. #running-req: 1, #token: 745, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.02, #queue-req: 0, 
[2025-09-30 13:46:23] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:23] Prefill batch. #new-seq: 1, #new-token: 79, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:23] Decode batch. #running-req: 1, #token: 286, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.17, #queue-req: 0, 
[2025-09-30 13:46:24] Decode batch. #running-req: 1, #token: 326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.88, #queue-req: 0, 
[2025-09-30 13:46:24] Decode batch. #running-req: 1, #token: 366, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.74, #queue-req: 0, 
[2025-09-30 13:46:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:24] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:24] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:24] Prefill batch. #new-seq: 1, #new-token: 551, #cached-token: 72, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:25] Decode batch. #running-req: 1, #token: 644, token usage: 0.01, cuda graph: True, gen throughput (token/s): 64.58, #queue-req: 0, 
[2025-09-30 13:46:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:25] Prefill batch. #new-seq: 1, #new-token: 644, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:25] Prefill batch. #new-seq: 1, #new-token: 98, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:25] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:25] Prefill batch. #new-seq: 1, #new-token: 88, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:25] Decode batch. #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 45.92, #queue-req: 0, 
[2025-09-30 13:46:26] Decode batch. #running-req: 1, #token: 329, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.85, #queue-req: 0, 
[2025-09-30 13:46:26] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.71, #queue-req: 0, 
[2025-09-30 13:46:26] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:27] Prefill batch. #new-seq: 1, #new-token: 648, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:27] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:27] Prefill batch. #new-seq: 1, #new-token: 556, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:27] Decode batch. #running-req: 1, #token: 757, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.69, #queue-req: 0, 
[2025-09-30 13:46:27] Decode batch. #running-req: 1, #token: 797, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.59, #queue-req: 0, 
[2025-09-30 13:46:27] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:27] Prefill batch. #new-seq: 1, #new-token: 98, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:28] Decode batch. #running-req: 1, #token: 330, token usage: 0.01, cuda graph: True, gen throughput (token/s): 126.26, #queue-req: 0, 
[2025-09-30 13:46:28] Decode batch. #running-req: 1, #token: 370, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.76, #queue-req: 0, 
[2025-09-30 13:46:28] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:28] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:28] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:28] Prefill batch. #new-seq: 1, #new-token: 552, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:28] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:29] Prefill batch. #new-seq: 1, #new-token: 1485, #cached-token: 343, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:29] Decode batch. #running-req: 1, #token: 1832, token usage: 0.03, cuda graph: True, gen throughput (token/s): 33.45, #queue-req: 0, 
[2025-09-30 13:46:29] Decode batch. #running-req: 1, #token: 1872, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.41, #queue-req: 0, 
[2025-09-30 13:46:30] Decode batch. #running-req: 1, #token: 1912, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.45, #queue-req: 0, 
[2025-09-30 13:46:30] Decode batch. #running-req: 1, #token: 1952, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.41, #queue-req: 0, 
[2025-09-30 13:46:30] Decode batch. #running-req: 1, #token: 1992, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.45, #queue-req: 0, 
[2025-09-30 13:46:31] Decode batch. #running-req: 1, #token: 2032, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.46, #queue-req: 0, 
[2025-09-30 13:46:31] Decode batch. #running-req: 1, #token: 2072, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.16, #queue-req: 0, 
[2025-09-30 13:46:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:31] Prefill batch. #new-seq: 1, #new-token: 1485, #cached-token: 323, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:31] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:31] Prefill batch. #new-seq: 1, #new-token: 501, #cached-token: 784, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:31] Decode batch. #running-req: 1, #token: 1305, token usage: 0.02, cuda graph: True, gen throughput (token/s): 95.16, #queue-req: 0, 
[2025-09-30 13:46:32] Decode batch. #running-req: 1, #token: 1345, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.86, #queue-req: 0, 
[2025-09-30 13:46:32] Decode batch. #running-req: 1, #token: 1385, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.77, #queue-req: 0, 
[2025-09-30 13:46:32] Decode batch. #running-req: 1, #token: 1425, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.71, #queue-req: 0, 
[2025-09-30 13:46:33] Decode batch. #running-req: 1, #token: 1465, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.65, #queue-req: 0, 
[2025-09-30 13:46:33] Decode batch. #running-req: 1, #token: 1505, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.66, #queue-req: 0, 
[2025-09-30 13:46:33] Decode batch. #running-req: 1, #token: 1545, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.55, #queue-req: 0, 
[2025-09-30 13:46:34] Decode batch. #running-req: 1, #token: 1585, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.19, #queue-req: 0, 
[2025-09-30 13:46:34] Decode batch. #running-req: 1, #token: 1625, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.12, #queue-req: 0, 
[2025-09-30 13:46:34] Decode batch. #running-req: 1, #token: 1665, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.04, #queue-req: 0, 
[2025-09-30 13:46:34] Decode batch. #running-req: 1, #token: 1705, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.07, #queue-req: 0, 
[2025-09-30 13:46:35] Decode batch. #running-req: 1, #token: 1745, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.07, #queue-req: 0, 
[2025-09-30 13:46:35] Decode batch. #running-req: 1, #token: 1785, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.03, #queue-req: 0, 
[2025-09-30 13:46:35] Decode batch. #running-req: 1, #token: 1825, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.70, #queue-req: 0, 
[2025-09-30 13:46:36] Decode batch. #running-req: 1, #token: 1865, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.56, #queue-req: 0, 
[2025-09-30 13:46:36] Decode batch. #running-req: 1, #token: 1905, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.50, #queue-req: 0, 
[2025-09-30 13:46:36] Decode batch. #running-req: 1, #token: 1945, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.37, #queue-req: 0, 
[2025-09-30 13:46:37] Decode batch. #running-req: 1, #token: 1985, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.42, #queue-req: 0, 
[2025-09-30 13:46:37] Decode batch. #running-req: 1, #token: 2025, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.38, #queue-req: 0, 
[2025-09-30 13:46:37] Decode batch. #running-req: 1, #token: 2065, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.18, #queue-req: 0, 
[2025-09-30 13:46:38] Decode batch. #running-req: 1, #token: 2105, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.89, #queue-req: 0, 
[2025-09-30 13:46:38] Decode batch. #running-req: 1, #token: 2145, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:46:38] Decode batch. #running-req: 1, #token: 2185, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.81, #queue-req: 0, 
[2025-09-30 13:46:39] Decode batch. #running-req: 1, #token: 2225, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.86, #queue-req: 0, 
[2025-09-30 13:46:39] Decode batch. #running-req: 1, #token: 2265, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.80, #queue-req: 0, 
[2025-09-30 13:46:39] Decode batch. #running-req: 1, #token: 2305, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.76, #queue-req: 0, 
[2025-09-30 13:46:39] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:40] Prefill batch. #new-seq: 1, #new-token: 1024, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:40] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:40] Prefill batch. #new-seq: 1, #new-token: 475, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:40] Decode batch. #running-req: 1, #token: 663, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.57, #queue-req: 0, 
[2025-09-30 13:46:40] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:40] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:41] Decode batch. #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.69, #queue-req: 0, 
[2025-09-30 13:46:41] Decode batch. #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.89, #queue-req: 0, 
[2025-09-30 13:46:41] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:41] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.71, #queue-req: 0, 
[2025-09-30 13:46:42] Prefill batch. #new-seq: 1, #new-token: 1153, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:42] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:42] Prefill batch. #new-seq: 1, #new-token: 683, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:42] Decode batch. #running-req: 1, #token: 886, token usage: 0.01, cuda graph: True, gen throughput (token/s): 42.97, #queue-req: 0, 
[2025-09-30 13:46:42] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:42] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:42] Decode batch. #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.65, #queue-req: 0, 
[2025-09-30 13:46:43] Decode batch. #running-req: 1, #token: 313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.89, #queue-req: 0, 
[2025-09-30 13:46:43] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:44] Prefill batch. #new-seq: 1, #new-token: 2569, #cached-token: 340, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:44] Decode batch. #running-req: 1, #token: 2912, token usage: 0.05, cuda graph: True, gen throughput (token/s): 42.12, #queue-req: 0, 
[2025-09-30 13:46:44] Decode batch. #running-req: 1, #token: 2952, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.98, #queue-req: 0, 
[2025-09-30 13:46:44] Decode batch. #running-req: 1, #token: 2992, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.02, #queue-req: 0, 
[2025-09-30 13:46:45] Decode batch. #running-req: 1, #token: 3032, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.02, #queue-req: 0, 
[2025-09-30 13:46:45] Decode batch. #running-req: 1, #token: 3072, token usage: 0.05, cuda graph: True, gen throughput (token/s): 126.00, #queue-req: 0, 
[2025-09-30 13:46:45] Decode batch. #running-req: 1, #token: 3112, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.51, #queue-req: 0, 
[2025-09-30 13:46:46] Decode batch. #running-req: 1, #token: 3152, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.47, #queue-req: 0, 
[2025-09-30 13:46:46] Decode batch. #running-req: 1, #token: 3192, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.44, #queue-req: 0, 
[2025-09-30 13:46:46] Decode batch. #running-req: 1, #token: 3232, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.46, #queue-req: 0, 
[2025-09-30 13:46:47] Decode batch. #running-req: 1, #token: 3272, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.45, #queue-req: 0, 
[2025-09-30 13:46:47] Decode batch. #running-req: 1, #token: 3312, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.43, #queue-req: 0, 
[2025-09-30 13:46:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:47] Prefill batch. #new-seq: 1, #new-token: 2569, #cached-token: 320, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:47] Decode batch. #running-req: 1, #token: 2923, token usage: 0.05, cuda graph: True, gen throughput (token/s): 88.59, #queue-req: 0, 
[2025-09-30 13:46:47] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:47] Prefill batch. #new-seq: 1, #new-token: 699, #cached-token: 835, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:48] Decode batch. #running-req: 1, #token: 1554, token usage: 0.02, cuda graph: True, gen throughput (token/s): 112.14, #queue-req: 0, 
[2025-09-30 13:46:48] Decode batch. #running-req: 1, #token: 1594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 129.16, #queue-req: 0, 
[2025-09-30 13:46:48] Decode batch. #running-req: 1, #token: 1634, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.08, #queue-req: 0, 
[2025-09-30 13:46:49] Decode batch. #running-req: 1, #token: 1674, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.05, #queue-req: 0, 
[2025-09-30 13:46:49] Decode batch. #running-req: 1, #token: 1714, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.00, #queue-req: 0, 
[2025-09-30 13:46:49] Decode batch. #running-req: 1, #token: 1754, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.03, #queue-req: 0, 
[2025-09-30 13:46:49] Decode batch. #running-req: 1, #token: 1794, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.00, #queue-req: 0, 
[2025-09-30 13:46:50] Decode batch. #running-req: 1, #token: 1834, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.53, #queue-req: 0, 
[2025-09-30 13:46:50] Decode batch. #running-req: 1, #token: 1874, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.53, #queue-req: 0, 
[2025-09-30 13:46:50] Decode batch. #running-req: 1, #token: 1914, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.44, #queue-req: 0, 
[2025-09-30 13:46:51] Decode batch. #running-req: 1, #token: 1954, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.41, #queue-req: 0, 
[2025-09-30 13:46:51] Decode batch. #running-req: 1, #token: 1994, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:46:51] Decode batch. #running-req: 1, #token: 2034, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.43, #queue-req: 0, 
[2025-09-30 13:46:52] Decode batch. #running-req: 1, #token: 2074, token usage: 0.03, cuda graph: True, gen throughput (token/s): 128.17, #queue-req: 0, 
[2025-09-30 13:46:52] Decode batch. #running-req: 1, #token: 2114, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.92, #queue-req: 0, 
[2025-09-30 13:46:52] Decode batch. #running-req: 1, #token: 2154, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.87, #queue-req: 0, 
[2025-09-30 13:46:53] Decode batch. #running-req: 1, #token: 2194, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.90, #queue-req: 0, 
[2025-09-30 13:46:53] Decode batch. #running-req: 1, #token: 2234, token usage: 0.03, cuda graph: True, gen throughput (token/s): 127.80, #queue-req: 0, 
[2025-09-30 13:46:53] Decode batch. #running-req: 1, #token: 2274, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.86, #queue-req: 0, 
[2025-09-30 13:46:54] Decode batch. #running-req: 1, #token: 2314, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.58, #queue-req: 0, 
[2025-09-30 13:46:54] Decode batch. #running-req: 1, #token: 2354, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.33, #queue-req: 0, 
[2025-09-30 13:46:54] Decode batch. #running-req: 1, #token: 2394, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.31, #queue-req: 0, 
[2025-09-30 13:46:55] Decode batch. #running-req: 1, #token: 2434, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.23, #queue-req: 0, 
[2025-09-30 13:46:55] Decode batch. #running-req: 1, #token: 2474, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.23, #queue-req: 0, 
[2025-09-30 13:46:55] Decode batch. #running-req: 1, #token: 2514, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.19, #queue-req: 0, 
[2025-09-30 13:46:55] Decode batch. #running-req: 1, #token: 2554, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.19, #queue-req: 0, 
[2025-09-30 13:46:56] Decode batch. #running-req: 1, #token: 2594, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.85, #queue-req: 0, 
[2025-09-30 13:46:56] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:57] Prefill batch. #new-seq: 1, #new-token: 1068, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:57] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:57] Prefill batch. #new-seq: 1, #new-token: 391, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:57] Decode batch. #running-req: 1, #token: 574, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.96, #queue-req: 0, 
[2025-09-30 13:46:57] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:57] Prefill batch. #new-seq: 1, #new-token: 73, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:57] Decode batch. #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.90, #queue-req: 0, 
[2025-09-30 13:46:57] Decode batch. #running-req: 1, #token: 331, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.86, #queue-req: 0, 
[2025-09-30 13:46:58] Decode batch. #running-req: 1, #token: 371, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.73, #queue-req: 0, 
[2025-09-30 13:46:58] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:58] Prefill batch. #new-seq: 1, #new-token: 761, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:58] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:58] Prefill batch. #new-seq: 1, #new-token: 378, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:59] Decode batch. #running-req: 1, #token: 575, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.71, #queue-req: 0, 
[2025-09-30 13:46:59] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:46:59] Prefill batch. #new-seq: 1, #new-token: 178, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:46:59] Decode batch. #running-req: 1, #token: 382, token usage: 0.01, cuda graph: True, gen throughput (token/s): 123.75, #queue-req: 0, 
[2025-09-30 13:46:59] Decode batch. #running-req: 1, #token: 422, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.41, #queue-req: 0, 
[2025-09-30 13:47:00] Decode batch. #running-req: 1, #token: 462, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.34, #queue-req: 0, 
[2025-09-30 13:47:00] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:00] Prefill batch. #new-seq: 1, #new-token: 745, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:00] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:00] Prefill batch. #new-seq: 1, #new-token: 375, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:01] Decode batch. #running-req: 1, #token: 561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.10, #queue-req: 0, 
[2025-09-30 13:47:01] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:01] Prefill batch. #new-seq: 1, #new-token: 86, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:01] Decode batch. #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.11, #queue-req: 0, 
[2025-09-30 13:47:01] Decode batch. #running-req: 1, #token: 333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.78, #queue-req: 0, 
[2025-09-30 13:47:01] Decode batch. #running-req: 1, #token: 373, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.67, #queue-req: 0, 
[2025-09-30 13:47:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:02] Prefill batch. #new-seq: 1, #new-token: 569, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:02] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:02] Prefill batch. #new-seq: 1, #new-token: 202, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:02] Decode batch. #running-req: 1, #token: 390, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.08, #queue-req: 0, 
[2025-09-30 13:47:03] Decode batch. #running-req: 1, #token: 430, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.85, #queue-req: 0, 
[2025-09-30 13:47:03] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:03] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:03] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.17, #queue-req: 0, 
[2025-09-30 13:47:03] Decode batch. #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.16, #queue-req: 0, 
[2025-09-30 13:47:04] Decode batch. #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.83, #queue-req: 0, 
[2025-09-30 13:47:04] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:04] Prefill batch. #new-seq: 1, #new-token: 515, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:04] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:04] Prefill batch. #new-seq: 1, #new-token: 321, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:04] Decode batch. #running-req: 1, #token: 509, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.02, #queue-req: 0, 
[2025-09-30 13:47:05] Decode batch. #running-req: 1, #token: 549, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.84, #queue-req: 0, 
[2025-09-30 13:47:05] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:05] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:05] Decode batch. #running-req: 1, #token: 236, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.72, #queue-req: 0, 
[2025-09-30 13:47:05] Decode batch. #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.03, #queue-req: 0, 
[2025-09-30 13:47:06] Decode batch. #running-req: 1, #token: 316, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.80, #queue-req: 0, 
[2025-09-30 13:47:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:06] Prefill batch. #new-seq: 1, #new-token: 697, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:06] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:06] Prefill batch. #new-seq: 1, #new-token: 384, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:07] Decode batch. #running-req: 1, #token: 562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-09-30 13:47:07] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:07] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:07] Decode batch. #running-req: 1, #token: 222, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.57, #queue-req: 0, 
[2025-09-30 13:47:07] Decode batch. #running-req: 1, #token: 262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.19, #queue-req: 0, 
[2025-09-30 13:47:07] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:08] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 69, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:08] Decode batch. #running-req: 1, #token: 82, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.40, #queue-req: 0, 
[2025-09-30 13:47:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:08] Prefill batch. #new-seq: 1, #new-token: 379, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:08] Decode batch. #running-req: 1, #token: 464, token usage: 0.01, cuda graph: True, gen throughput (token/s): 121.47, #queue-req: 0, 
[2025-09-30 13:47:08] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:09] Prefill batch. #new-seq: 1, #new-token: 638, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:09] Prefill batch. #new-seq: 1, #new-token: 261, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:09] Decode batch. #running-req: 1, #token: 445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.79, #queue-req: 0, 
[2025-09-30 13:47:09] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:09] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:09] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.02, #queue-req: 0, 
[2025-09-30 13:47:10] Decode batch. #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 134.13, #queue-req: 0, 
[2025-09-30 13:47:10] Decode batch. #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.84, #queue-req: 0, 
[2025-09-30 13:47:10] INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:47:11] Prefill batch. #new-seq: 1, #new-token: 2054, #cached-token: 339, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:47:11] Decode batch. #running-req: 1, #token: 2417, token usage: 0.04, cuda graph: True, gen throughput (token/s): 42.26, #queue-req: 0, 
[2025-09-30 13:47:11] Decode batch. #running-req: 1, #token: 2457, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.16, #queue-req: 0, 
[2025-09-30 13:47:11] Decode batch. #running-req: 1, #token: 2497, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.12, #queue-req: 0, 
[2025-09-30 13:47:12] Decode batch. #running-req: 1, #token: 2537, token usage: 0.04, cuda graph: True, gen throughput (token/s): 127.11, #queue-req: 0, 
[2025-09-30 13:47:12] Decode batch. #running-req: 1, #token: 2577, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.96, #queue-req: 0, 
[2025-09-30 13:47:12] Decode batch. #running-req: 1, #token: 2617, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.62, #queue-req: 0, 
[2025-09-30 13:47:13] Decode batch. #running-req: 1, #token: 2657, token usage: 0.04, cuda graph: True, gen throughput (token/s): 126.62, #queue-req: 0, 
Process Process-1:
Process Process-2:
Traceback (most recent call last):
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 2724, in run_scheduler_process
    scheduler.event_loop_overlap()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 897, in event_loop_overlap
    self.self_check_during_idle()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 1469, in self_check_during_idle
    self.check_memory()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 1492, in check_memory
    _, _, available_size, evictable_size = self._get_token_info()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 1572, in _get_token_info
    available_size = self.token_to_kv_pool_allocator.available_size()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/mem_cache/allocator.py", line 141, in available_size
    def available_size(self):
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/detokenizer_manager.py", line 298, in run_detokenizer_process
    manager.event_loop()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/detokenizer_manager.py", line 117, in event_loop
    recv_obj = self.recv_from_scheduler.recv_pyobj()
  File "/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/zmq/sugar/socket.py", line 989, in recv_pyobj
    msg = self.recv(flags)
  File "zmq/backend/cython/_zmq.py", line 1218, in zmq.backend.cython._zmq.Socket.recv
  File "zmq/backend/cython/_zmq.py", line 1253, in zmq.backend.cython._zmq.Socket.recv
  File "zmq/backend/cython/_zmq.py", line 1408, in zmq.backend.cython._zmq._recv_copy
  File "zmq/backend/cython/_zmq.py", line 179, in zmq.backend.cython._zmq._check_rc
KeyboardInterrupt
[2025-09-30 13:47:15] INFO:     Shutting down
[2025-09-30 13:47:15] INFO:     Waiting for application shutdown.
[2025-09-30 13:47:15] INFO:     Application shutdown complete.
[2025-09-30 13:47:15] INFO:     Finished server process [2752152]
