/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W0930 13:54:33.373996 2768455 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W0930 13:54:33.373996 2768455 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[2025-09-30 13:54:33] server_args=ServerArgs(model_path='/mnt/data/models/Llama-3.2-3B-Instruct', tokenizer_path='/mnt/data/models/Llama-3.2-3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=30003, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.98, max_running_requests=50, max_queued_requests=9223372036854775807, max_total_tokens=64000, chunked_prefill_size=4096, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=153836219, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-customer-labels', tokenizer_metrics_allowed_customer_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='LLAMA', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=True, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=True, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_cutedsl_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-09-30 13:54:34] Using default HuggingFace chat template with detected content format: string
/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/shm/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W0930 13:54:39.870558 2768744 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W0930 13:54:39.870558 2768744 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
W0930 13:54:40.188240 2768743 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W0930 13:54:40.188240 2768743 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-09-30 13:54:40] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-09-30 13:54:41] Init torch distributed ends. mem usage=0.00 GB
[2025-09-30 13:54:41] CUDA-fused xIELU not available (No module named 'xielu') – falling back to a Python version.
For CUDA xIELU (experimental), `pip install git+https://github.com/nickjbrowning/XIELU`
[2025-09-30 13:54:41] MOE_RUNNER_BACKEND is not initialized, using triton backend
[2025-09-30 13:54:41] Load weight begin. avail mem=23.13 GB
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.31it/s]

[2025-09-30 13:54:43] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=17.01 GB, mem usage=6.12 GB.
[2025-09-30 13:54:43] KV Cache is allocated. #tokens: 64000, K size: 3.42 GB, V size: 3.42 GB
[2025-09-30 13:54:43] Memory pool end. avail mem=10.14 GB
[2025-09-30 13:54:43] Capture cuda graph begin. This can take up to several minutes. avail mem=10.10 GB
[2025-09-30 13:54:43] Capture cuda graph bs [1, 2, 4, 8]
  0%|          | 0/4 [00:00<?, ?it/s]Capturing batches (bs=8 avail_mem=10.10 GB):   0%|          | 0/4 [00:00<?, ?it/s]Capturing batches (bs=8 avail_mem=10.10 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.75it/s]Capturing batches (bs=4 avail_mem=10.05 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.75it/s]Capturing batches (bs=2 avail_mem=10.04 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.75it/s]Capturing batches (bs=1 avail_mem=10.04 GB):  25%|██▌       | 1/4 [00:00<00:01,  1.75it/s]Capturing batches (bs=1 avail_mem=10.04 GB): 100%|██████████| 4/4 [00:00<00:00,  7.10it/s]Capturing batches (bs=1 avail_mem=10.04 GB): 100%|██████████| 4/4 [00:00<00:00,  5.77it/s]
[2025-09-30 13:54:44] Capture cuda graph end. Time elapsed: 1.12 s. mem usage=0.07 GB. avail mem=10.04 GB.
[2025-09-30 13:54:44] max_total_num_tokens=64000, chunked_prefill_size=4096, max_prefill_tokens=16384, max_running_requests=50, context_len=131072, available_gpu_mem=10.04 GB
[2025-09-30 13:54:45] INFO:     Started server process [2768455]
[2025-09-30 13:54:45] INFO:     Waiting for application startup.
[2025-09-30 13:54:45] INFO:     Application startup complete.
[2025-09-30 13:54:45] INFO:     Uvicorn running on http://0.0.0.0:30003 (Press CTRL+C to quit)
[2025-09-30 13:54:46] INFO:     127.0.0.1:44012 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-09-30 13:54:46] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:46] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[2025-09-30 13:54:46] The server is fired up and ready to roll!
[2025-09-30 13:54:55] Prefill batch. #new-seq: 1, #new-token: 465, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:56] Decode batch. #running-req: 1, #token: 499, token usage: 0.01, cuda graph: True, gen throughput (token/s): 3.58, #queue-req: 0, 
[2025-09-30 13:54:56] Decode batch. #running-req: 1, #token: 539, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.64, #queue-req: 0, 
[2025-09-30 13:54:56] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:56] Prefill batch. #new-seq: 1, #new-token: 325, #cached-token: 27, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:56] Decode batch. #running-req: 1, #token: 374, token usage: 0.01, cuda graph: True, gen throughput (token/s): 119.78, #queue-req: 0, 
[2025-09-30 13:54:56] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:57] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:57] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:57] Prefill batch. #new-seq: 1, #new-token: 303, #cached-token: 69, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:57] Decode batch. #running-req: 1, #token: 390, token usage: 0.01, cuda graph: True, gen throughput (token/s): 62.24, #queue-req: 0, 
[2025-09-30 13:54:57] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:57] Prefill batch. #new-seq: 1, #new-token: 967, #cached-token: 29, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:57] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:57] Prefill batch. #new-seq: 1, #new-token: 623, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:57] Decode batch. #running-req: 1, #token: 816, token usage: 0.01, cuda graph: True, gen throughput (token/s): 102.58, #queue-req: 0, 
[2025-09-30 13:54:57] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:57] Prefill batch. #new-seq: 1, #new-token: 72, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:57] Decode batch. #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.73, #queue-req: 0, 
[2025-09-30 13:54:58] Decode batch. #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.40, #queue-req: 0, 
[2025-09-30 13:54:58] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:58] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:58] Decode batch. #running-req: 1, #token: 85, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.46, #queue-req: 0, 
[2025-09-30 13:54:58] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:58] Prefill batch. #new-seq: 1, #new-token: 619, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:58] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:58] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:59] Decode batch. #running-req: 1, #token: 101, token usage: 0.00, cuda graph: True, gen throughput (token/s): 100.40, #queue-req: 0, 
[2025-09-30 13:54:59] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:59] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 689, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:59] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:59] Prefill batch. #new-seq: 1, #new-token: 1320, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:59] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:59] Prefill batch. #new-seq: 1, #new-token: 704, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:54:59] Decode batch. #running-req: 1, #token: 868, token usage: 0.01, cuda graph: True, gen throughput (token/s): 90.82, #queue-req: 0, 
[2025-09-30 13:54:59] Decode batch. #running-req: 1, #token: 908, token usage: 0.01, cuda graph: True, gen throughput (token/s): 129.98, #queue-req: 0, 
[2025-09-30 13:54:59] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:54:59] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:00] Decode batch. #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.54, #queue-req: 0, 
[2025-09-30 13:55:00] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.36, #queue-req: 0, 
[2025-09-30 13:55:00] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:00] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:00] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:00] Prefill batch. #new-seq: 1, #new-token: 700, #cached-token: 72, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:00] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:00] Prefill batch. #new-seq: 1, #new-token: 1123, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:00] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:00] Prefill batch. #new-seq: 1, #new-token: 427, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:00] Decode batch. #running-req: 1, #token: 598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 78.48, #queue-req: 0, 
[2025-09-30 13:55:01] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:01] Prefill batch. #new-seq: 1, #new-token: 81, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:01] Decode batch. #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.91, #queue-req: 0, 
[2025-09-30 13:55:01] Decode batch. #running-req: 1, #token: 323, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.41, #queue-req: 0, 
[2025-09-30 13:55:01] Decode batch. #running-req: 1, #token: 363, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.36, #queue-req: 0, 
[2025-09-30 13:55:01] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:01] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:02] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:02] Prefill batch. #new-seq: 1, #new-token: 423, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:02] Decode batch. #running-req: 1, #token: 497, token usage: 0.01, cuda graph: True, gen throughput (token/s): 105.88, #queue-req: 0, 
[2025-09-30 13:55:02] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:02] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:02] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:02] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 493, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:02] Decode batch. #running-req: 1, #token: 496, token usage: 0.01, cuda graph: True, gen throughput (token/s): 106.17, #queue-req: 0, 
[2025-09-30 13:55:02] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:02] Prefill batch. #new-seq: 1, #new-token: 1008, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:02] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:02] Prefill batch. #new-seq: 1, #new-token: 587, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:02] Decode batch. #running-req: 1, #token: 763, token usage: 0.01, cuda graph: True, gen throughput (token/s): 99.90, #queue-req: 0, 
[2025-09-30 13:55:03] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:03] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:03] Decode batch. #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.70, #queue-req: 0, 
[2025-09-30 13:55:03] Decode batch. #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.40, #queue-req: 0, 
[2025-09-30 13:55:03] Decode batch. #running-req: 1, #token: 345, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.37, #queue-req: 0, 
[2025-09-30 13:55:03] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:03] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:04] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:04] Prefill batch. #new-seq: 1, #new-token: 583, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:04] Decode batch. #running-req: 1, #token: 679, token usage: 0.01, cuda graph: True, gen throughput (token/s): 102.48, #queue-req: 0, 
[2025-09-30 13:55:04] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:04] Prefill batch. #new-seq: 1, #new-token: 989, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:04] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:04] Prefill batch. #new-seq: 1, #new-token: 408, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:04] Decode batch. #running-req: 1, #token: 606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 94.47, #queue-req: 0, 
[2025-09-30 13:55:05] Decode batch. #running-req: 1, #token: 646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.91, #queue-req: 0, 
[2025-09-30 13:55:05] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:05] Prefill batch. #new-seq: 1, #new-token: 62, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:05] Decode batch. #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.79, #queue-req: 0, 
[2025-09-30 13:55:05] Decode batch. #running-req: 1, #token: 333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.34, #queue-req: 0, 
[2025-09-30 13:55:05] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:05] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:05] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:05] Prefill batch. #new-seq: 1, #new-token: 404, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:05] Decode batch. #running-req: 1, #token: 498, token usage: 0.01, cuda graph: True, gen throughput (token/s): 114.81, #queue-req: 0, 
[2025-09-30 13:55:06] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:06] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:06] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:06] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 476, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:06] Decode batch. #running-req: 1, #token: 504, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.40, #queue-req: 0, 
[2025-09-30 13:55:06] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:06] Prefill batch. #new-seq: 1, #new-token: 889, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:06] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:06] Prefill batch. #new-seq: 1, #new-token: 486, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:06] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:06] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:06] Decode batch. #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 99.88, #queue-req: 0, 
[2025-09-30 13:55:07] Decode batch. #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.37, #queue-req: 0, 
[2025-09-30 13:55:07] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:07] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:07] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:07] Prefill batch. #new-seq: 1, #new-token: 482, #cached-token: 74, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:07] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:07] Prefill batch. #new-seq: 1, #new-token: 729, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:07] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:07] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 90.93, #queue-req: 0, 
[2025-09-30 13:55:07] Prefill batch. #new-seq: 1, #new-token: 251, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:07] Decode batch. #running-req: 1, #token: 453, token usage: 0.01, cuda graph: True, gen throughput (token/s): 125.03, #queue-req: 0, 
[2025-09-30 13:55:07] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:07] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:08] Decode batch. #running-req: 1, #token: 242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.22, #queue-req: 0, 
[2025-09-30 13:55:08] Decode batch. #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.52, #queue-req: 0, 
[2025-09-30 13:55:08] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:08] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:08] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:08] Prefill batch. #new-seq: 1, #new-token: 246, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:08] Decode batch. #running-req: 1, #token: 325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 110.66, #queue-req: 0, 
[2025-09-30 13:55:08] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:08] Prefill batch. #new-seq: 1, #new-token: 729, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:09] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:09] Prefill batch. #new-seq: 1, #new-token: 551, #cached-token: 152, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:09] Decode batch. #running-req: 1, #token: 723, token usage: 0.01, cuda graph: True, gen throughput (token/s): 93.83, #queue-req: 0, 
[2025-09-30 13:55:09] Decode batch. #running-req: 1, #token: 763, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.66, #queue-req: 0, 
[2025-09-30 13:55:09] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:09] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:09] Decode batch. #running-req: 1, #token: 238, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.82, #queue-req: 0, 
[2025-09-30 13:55:10] Decode batch. #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.63, #queue-req: 0, 
[2025-09-30 13:55:10] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:10] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:10] Decode batch. #running-req: 1, #token: 93, token usage: 0.00, cuda graph: True, gen throughput (token/s): 118.35, #queue-req: 0, 
[2025-09-30 13:55:10] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:10] Prefill batch. #new-seq: 1, #new-token: 482, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:10] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 116.73, #queue-req: 0, 
[2025-09-30 13:55:10] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:10] Prefill batch. #new-seq: 1, #new-token: 1139, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:10] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:10] Prefill batch. #new-seq: 1, #new-token: 659, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:11] Decode batch. #running-req: 1, #token: 860, token usage: 0.01, cuda graph: True, gen throughput (token/s): 87.42, #queue-req: 0, 
[2025-09-30 13:55:11] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:11] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:11] Decode batch. #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.72, #queue-req: 0, 
[2025-09-30 13:55:11] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:11] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:11] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, cuda graph: True, gen throughput (token/s): 118.60, #queue-req: 0, 
[2025-09-30 13:55:11] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:11] Prefill batch. #new-seq: 1, #new-token: 655, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:12] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:12] Prefill batch. #new-seq: 1, #new-token: 1204, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:12] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:12] Prefill batch. #new-seq: 1, #new-token: 551, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:12] Decode batch. #running-req: 1, #token: 723, token usage: 0.01, cuda graph: True, gen throughput (token/s): 81.61, #queue-req: 0, 
[2025-09-30 13:55:12] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:12] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:12] Decode batch. #running-req: 1, #token: 220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.32, #queue-req: 0, 
[2025-09-30 13:55:13] Decode batch. #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.75, #queue-req: 0, 
[2025-09-30 13:55:13] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:13] Prefill batch. #new-seq: 1, #new-token: 1309, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:13] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:13] Prefill batch. #new-seq: 1, #new-token: 766, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:13] Decode batch. #running-req: 1, #token: 931, token usage: 0.01, cuda graph: True, gen throughput (token/s): 84.38, #queue-req: 0, 
[2025-09-30 13:55:13] Decode batch. #running-req: 1, #token: 971, token usage: 0.02, cuda graph: True, gen throughput (token/s): 130.02, #queue-req: 0, 
[2025-09-30 13:55:14] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:14] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:14] Decode batch. #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.94, #queue-req: 0, 
[2025-09-30 13:55:14] Decode batch. #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.52, #queue-req: 0, 
[2025-09-30 13:55:14] Decode batch. #running-req: 1, #token: 321, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.37, #queue-req: 0, 
[2025-09-30 13:55:14] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:14] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:15] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:15] Prefill batch. #new-seq: 1, #new-token: 762, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:15] Decode batch. #running-req: 1, #token: 836, token usage: 0.01, cuda graph: True, gen throughput (token/s): 102.40, #queue-req: 0, 
[2025-09-30 13:55:15] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:15] Prefill batch. #new-seq: 1, #new-token: 1176, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:15] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:15] Prefill batch. #new-seq: 1, #new-token: 416, #cached-token: 163, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:15] Decode batch. #running-req: 1, #token: 595, token usage: 0.01, cuda graph: True, gen throughput (token/s): 88.85, #queue-req: 0, 
[2025-09-30 13:55:15] Decode batch. #running-req: 1, #token: 635, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.16, #queue-req: 0, 
[2025-09-30 13:55:15] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:15] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:16] Decode batch. #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.61, #queue-req: 0, 
[2025-09-30 13:55:16] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:16] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:16] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, cuda graph: True, gen throughput (token/s): 118.88, #queue-req: 0, 
[2025-09-30 13:55:16] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:16] Prefill batch. #new-seq: 1, #new-token: 411, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:16] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:16] Prefill batch. #new-seq: 1, #new-token: 748, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:16] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:16] Prefill batch. #new-seq: 1, #new-token: 340, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:16] Decode batch. #running-req: 1, #token: 517, token usage: 0.01, cuda graph: True, gen throughput (token/s): 85.34, #queue-req: 0, 
[2025-09-30 13:55:17] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:17] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:17] Decode batch. #running-req: 1, #token: 218, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.40, #queue-req: 0, 
[2025-09-30 13:55:17] Decode batch. #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.82, #queue-req: 0, 
[2025-09-30 13:55:17] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:17] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:17] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:17] Prefill batch. #new-seq: 1, #new-token: 336, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:17] Decode batch. #running-req: 1, #token: 424, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.56, #queue-req: 0, 
[2025-09-30 13:55:18] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:18] Prefill batch. #new-seq: 1, #new-token: 718, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:18] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:18] Prefill batch. #new-seq: 1, #new-token: 386, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:18] Decode batch. #running-req: 1, #token: 580, token usage: 0.01, cuda graph: True, gen throughput (token/s): 91.56, #queue-req: 0, 
[2025-09-30 13:55:18] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:18] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:18] Decode batch. #running-req: 1, #token: 228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.58, #queue-req: 0, 
[2025-09-30 13:55:19] Decode batch. #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.69, #queue-req: 0, 
[2025-09-30 13:55:19] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:19] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 68, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:19] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:19] Prefill batch. #new-seq: 1, #new-token: 382, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:19] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:19] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:19] Decode batch. #running-req: 1, #token: 92, token usage: 0.00, cuda graph: True, gen throughput (token/s): 88.43, #queue-req: 0, 
[2025-09-30 13:55:19] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:19] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 452, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:19] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:19] Prefill batch. #new-seq: 1, #new-token: 724, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:19] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:19] Prefill batch. #new-seq: 1, #new-token: 346, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:19] Decode batch. #running-req: 1, #token: 523, token usage: 0.01, cuda graph: True, gen throughput (token/s): 91.44, #queue-req: 0, 
[2025-09-30 13:55:20] Decode batch. #running-req: 1, #token: 563, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.40, #queue-req: 0, 
[2025-09-30 13:55:20] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:20] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:20] Decode batch. #running-req: 1, #token: 232, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.33, #queue-req: 0, 
[2025-09-30 13:55:20] Decode batch. #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.67, #queue-req: 0, 
[2025-09-30 13:55:21] Decode batch. #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.38, #queue-req: 0, 
[2025-09-30 13:55:21] Decode batch. #running-req: 1, #token: 352, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.25, #queue-req: 0, 
[2025-09-30 13:55:21] Decode batch. #running-req: 1, #token: 392, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.16, #queue-req: 0, 
[2025-09-30 13:55:22] Decode batch. #running-req: 1, #token: 432, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.97, #queue-req: 0, 
[2025-09-30 13:55:22] Decode batch. #running-req: 1, #token: 472, token usage: 0.01, cuda graph: True, gen throughput (token/s): 132.84, #queue-req: 0, 
[2025-09-30 13:55:22] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:22] Prefill batch. #new-seq: 1, #new-token: 376, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:22] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:22] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 161, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:22] Decode batch. #running-req: 1, #token: 201, token usage: 0.00, cuda graph: True, gen throughput (token/s): 115.96, #queue-req: 0, 
[2025-09-30 13:55:22] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:22] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 194, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:22] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:23] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:23] Decode batch. #running-req: 1, #token: 81, token usage: 0.00, cuda graph: True, gen throughput (token/s): 112.74, #queue-req: 0, 
[2025-09-30 13:55:23] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:23] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:23] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:23] Prefill batch. #new-seq: 1, #new-token: 556, #cached-token: 76, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:23] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:23] Prefill batch. #new-seq: 1, #new-token: 523, #cached-token: 164, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:23] Decode batch. #running-req: 1, #token: 699, token usage: 0.01, cuda graph: True, gen throughput (token/s): 92.84, #queue-req: 0, 
[2025-09-30 13:55:23] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:23] Prefill batch. #new-seq: 1, #new-token: 86, #cached-token: 197, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:23] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.54, #queue-req: 0, 
[2025-09-30 13:55:24] Decode batch. #running-req: 1, #token: 340, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.22, #queue-req: 0, 
[2025-09-30 13:55:24] Decode batch. #running-req: 1, #token: 380, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.19, #queue-req: 0, 
[2025-09-30 13:55:24] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:24] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:24] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:24] Prefill batch. #new-seq: 1, #new-token: 519, #cached-token: 74, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:24] Decode batch. #running-req: 1, #token: 602, token usage: 0.01, cuda graph: True, gen throughput (token/s): 100.76, #queue-req: 0, 
[2025-09-30 13:55:24] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:24] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:25] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:25] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 592, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:25] Decode batch. #running-req: 1, #token: 597, token usage: 0.01, cuda graph: True, gen throughput (token/s): 95.46, #queue-req: 0, 
[2025-09-30 13:55:25] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:25] Prefill batch. #new-seq: 1, #new-token: 1064, #cached-token: 79, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:25] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:25] Prefill batch. #new-seq: 1, #new-token: 590, #cached-token: 152, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:25] Decode batch. #running-req: 1, #token: 761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 77.38, #queue-req: 0, 
[2025-09-30 13:55:26] Decode batch. #running-req: 1, #token: 801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 130.31, #queue-req: 0, 
[2025-09-30 13:55:26] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:26] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 196, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:26] Decode batch. #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 120.91, #queue-req: 0, 
[2025-09-30 13:55:26] Decode batch. #running-req: 1, #token: 330, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.31, #queue-req: 0, 
[2025-09-30 13:55:26] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:27] Prefill batch. #new-seq: 1, #new-token: 905, #cached-token: 78, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:27] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:27] Prefill batch. #new-seq: 1, #new-token: 362, #cached-token: 165, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:27] Decode batch. #running-req: 1, #token: 538, token usage: 0.01, cuda graph: True, gen throughput (token/s): 80.76, #queue-req: 0, 
[2025-09-30 13:55:27] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:27] Prefill batch. #new-seq: 1, #new-token: 70, #cached-token: 198, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:27] Decode batch. #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.36, #queue-req: 0, 
[2025-09-30 13:55:27] Decode batch. #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.38, #queue-req: 0, 
[2025-09-30 13:55:28] Decode batch. #running-req: 1, #token: 351, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.27, #queue-req: 0, 
[2025-09-30 13:55:28] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:28] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:28] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:28] Prefill batch. #new-seq: 1, #new-token: 358, #cached-token: 75, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:28] Decode batch. #running-req: 1, #token: 455, token usage: 0.01, cuda graph: True, gen throughput (token/s): 89.72, #queue-req: 0, 
[2025-09-30 13:55:28] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:28] Prefill batch. #new-seq: 1, #new-token: 651, #cached-token: 80, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:28] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:28] Prefill batch. #new-seq: 1, #new-token: 291, #cached-token: 167, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:28] Decode batch. #running-req: 1, #token: 493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 91.88, #queue-req: 0, 
[2025-09-30 13:55:29] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:29] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:29] Decode batch. #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 122.62, #queue-req: 0, 
[2025-09-30 13:55:29] Decode batch. #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.29, #queue-req: 0, 
[2025-09-30 13:55:29] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:29] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:29] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:29] Prefill batch. #new-seq: 1, #new-token: 287, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:29] Decode batch. #running-req: 1, #token: 372, token usage: 0.01, cuda graph: True, gen throughput (token/s): 104.52, #queue-req: 0, 
[2025-09-30 13:55:30] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:30] Prefill batch. #new-seq: 1, #new-token: 2869, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:30] Decode batch. #running-req: 1, #token: 2922, token usage: 0.05, cuda graph: True, gen throughput (token/s): 68.42, #queue-req: 0, 
[2025-09-30 13:55:30] Decode batch. #running-req: 1, #token: 2962, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.49, #queue-req: 0, 
[2025-09-30 13:55:31] Decode batch. #running-req: 1, #token: 3002, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.46, #queue-req: 0, 
[2025-09-30 13:55:31] Decode batch. #running-req: 1, #token: 3042, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.47, #queue-req: 0, 
[2025-09-30 13:55:31] Decode batch. #running-req: 1, #token: 3082, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.31, #queue-req: 0, 
[2025-09-30 13:55:32] Decode batch. #running-req: 1, #token: 3122, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.00, #queue-req: 0, 
[2025-09-30 13:55:32] Decode batch. #running-req: 1, #token: 3162, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.98, #queue-req: 0, 
[2025-09-30 13:55:32] Decode batch. #running-req: 1, #token: 3202, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.95, #queue-req: 0, 
[2025-09-30 13:55:33] Decode batch. #running-req: 1, #token: 3242, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.95, #queue-req: 0, 
[2025-09-30 13:55:33] Decode batch. #running-req: 1, #token: 3282, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.92, #queue-req: 0, 
[2025-09-30 13:55:33] Decode batch. #running-req: 1, #token: 3322, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.93, #queue-req: 0, 
[2025-09-30 13:55:34] Decode batch. #running-req: 1, #token: 3362, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.55, #queue-req: 0, 
[2025-09-30 13:55:34] Decode batch. #running-req: 1, #token: 3402, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.42, #queue-req: 0, 
[2025-09-30 13:55:34] Decode batch. #running-req: 1, #token: 3442, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.47, #queue-req: 0, 
[2025-09-30 13:55:35] Decode batch. #running-req: 1, #token: 3482, token usage: 0.05, cuda graph: True, gen throughput (token/s): 124.46, #queue-req: 0, 
[2025-09-30 13:55:35] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:35] Prefill batch. #new-seq: 1, #new-token: 2849, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:35] Decode batch. #running-req: 1, #token: 2907, token usage: 0.05, cuda graph: True, gen throughput (token/s): 85.70, #queue-req: 0, 
[2025-09-30 13:55:35] Decode batch. #running-req: 1, #token: 2947, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.53, #queue-req: 0, 
[2025-09-30 13:55:36] Decode batch. #running-req: 1, #token: 2987, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.54, #queue-req: 0, 
[2025-09-30 13:55:36] Decode batch. #running-req: 1, #token: 3027, token usage: 0.05, cuda graph: True, gen throughput (token/s): 125.50, #queue-req: 0, 
[2025-09-30 13:55:36] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:36] Prefill batch. #new-seq: 1, #new-token: 502, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:36] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:36] Prefill batch. #new-seq: 1, #new-token: 279, #cached-token: 153, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:37] Decode batch. #running-req: 1, #token: 458, token usage: 0.01, cuda graph: True, gen throughput (token/s): 74.05, #queue-req: 0, 
[2025-09-30 13:55:37] Decode batch. #running-req: 1, #token: 498, token usage: 0.01, cuda graph: True, gen throughput (token/s): 131.87, #queue-req: 0, 
[2025-09-30 13:55:37] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:37] Prefill batch. #new-seq: 1, #new-token: 47, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:37] Decode batch. #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.99, #queue-req: 0, 
[2025-09-30 13:55:37] Decode batch. #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.33, #queue-req: 0, 
[2025-09-30 13:55:38] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:38] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:38] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:38] Prefill batch. #new-seq: 1, #new-token: 214, #cached-token: 72, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:38] Decode batch. #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 92.25, #queue-req: 0, 
[2025-09-30 13:55:38] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:38] Prefill batch. #new-seq: 1, #new-token: 504, #cached-token: 77, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:38] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:38] Prefill batch. #new-seq: 1, #new-token: 293, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:38] Decode batch. #running-req: 1, #token: 479, token usage: 0.01, cuda graph: True, gen throughput (token/s): 92.05, #queue-req: 0, 
[2025-09-30 13:55:39] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:39] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 195, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-09-30 13:55:39] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.30, #queue-req: 0, 
[2025-09-30 13:55:39] Decode batch. #running-req: 1, #token: 340, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.30, #queue-req: 0, 
[2025-09-30 13:55:39] Decode batch. #running-req: 1, #token: 380, token usage: 0.01, cuda graph: True, gen throughput (token/s): 133.17, #queue-req: 0, 
[2025-09-30 13:55:39] INFO:     127.0.0.1:57520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-09-30 13:55:39] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 70, token usage: 0.00, #running-req: 0, #queue-req: 0, 
